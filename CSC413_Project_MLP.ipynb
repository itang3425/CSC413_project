{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd /content/CLIP\n",
        "!pip install ftfy\n",
        "import clip\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIT4Xnm9HKDI",
        "outputId": "871ce280-61b4-4130-89e0-1871eb54c83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 256, done.\u001b[K\n",
            "remote: Total 256 (delta 0), reused 0 (delta 0), pack-reused 256 (from 1)\u001b[K\n",
            "Receiving objects: 100% (256/256), 8.87 MiB | 12.65 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "/content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import clip\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Converts CSV data into text-image pairs using CLIP embeddings.\n",
        "    Matches character names from CSV to image filenames.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_file=None, clip_model=None, clip_preprocess=None, device=None,\n",
        "                 image_dir=None, base_prompt=\"Genshin-style character\", encoding='utf-8'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file: Path to CSV with character data (must have 'character_name' column)\n",
        "            clip_model: Pre-trained CLIP model for encoding\n",
        "            clip_preprocess: CLIP preprocessing function for images\n",
        "            device: torch device\n",
        "            image_dir: Directory containing character images (e.g., \"Albedo.png\")\n",
        "            base_prompt: Base text prompt (e.g., \"Genshin-style character\")\n",
        "            encoding: CSV file encoding\n",
        "        \"\"\"\n",
        "        self.clip_model = clip_model\n",
        "        self.clip_preprocess = clip_preprocess\n",
        "        self.device = device\n",
        "        self.base_prompt = base_prompt\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "        # Hard-coded Google Drive paths\n",
        "        if csv_file is None:\n",
        "            csv_file = \"/content/drive/MyDrive/filtered_output.csv\"\n",
        "        if image_dir is None:\n",
        "            image_dir = \"/content/drive/MyDrive/characters/\"\n",
        "\n",
        "        print(f\"Loading CSV from: {csv_file}\")\n",
        "        print(f\"Looking for images in: {image_dir}\")\n",
        "\n",
        "        # Read CSV with multiple encoding attempts\n",
        "        encodings_to_try = [encoding, 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "\n",
        "        for enc in encodings_to_try:\n",
        "            try:\n",
        "                self.df = pd.read_csv(csv_file, encoding=enc)\n",
        "                print(f\"✓ Successfully loaded CSV with encoding: {enc}\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                if enc == encodings_to_try[-1]:\n",
        "                    raise\n",
        "                continue\n",
        "            except FileNotFoundError:\n",
        "                raise FileNotFoundError(f\"CSV file not found at: {csv_file}\\nPlease check the path in Google Drive.\")\n",
        "\n",
        "        # Verify character_name column exists\n",
        "        if 'character_name' not in self.df.columns:\n",
        "            raise ValueError(f\"CSV must have 'character_name' column. Found: {list(self.df.columns)}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.df)} characters from CSV\")\n",
        "        print(f\"Columns: {list(self.df.columns)}\")\n",
        "\n",
        "        # Define the string columns that go through CLIP encoding\n",
        "        self.string_columns = ['character_name', 'constellation', 'affiliation']\n",
        "\n",
        "        # Define categorical columns\n",
        "        self.categorical_columns = ['region', 'vision', 'weapon_type', 'body_figure']\n",
        "\n",
        "        # Build categorical mappings\n",
        "        self.cat_mappings = {}\n",
        "        for col in self.categorical_columns:\n",
        "            if col in self.df.columns:\n",
        "                unique_values = self.df[col].unique()\n",
        "                self.cat_mappings[col] = {val: idx for idx, val in enumerate(unique_values)}\n",
        "                print(f\"{col}: {len(unique_values)} unique values - {unique_values}\")\n",
        "\n",
        "        # Find matching images for each character\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MATCHING CHARACTER NAMES TO IMAGE FILES\")\n",
        "        print(\"=\"*60)\n",
        "        self._match_images_to_characters()\n",
        "\n",
        "        # Pre-compute base text embedding\n",
        "        print(f\"\\nPre-computing base text embedding for: '{self.base_prompt}'\")\n",
        "        with torch.no_grad():\n",
        "            base_token = clip.tokenize([self.base_prompt], truncate=True).to(device)\n",
        "            self.base_text_embed = self.clip_model.encode_text(base_token)\n",
        "            self.base_text_embed = self.base_text_embed / self.base_text_embed.norm(dim=-1, keepdim=True)\n",
        "            self.base_text_embed = self.base_text_embed.cpu()\n",
        "\n",
        "        # Pre-compute CLIP embeddings for string inputs\n",
        "        print(\"\\nPre-computing CLIP TEXT embeddings for character attributes...\")\n",
        "        self._precompute_text_embeddings()\n",
        "\n",
        "        # Pre-compute target CLIP IMAGE embeddings\n",
        "        print(\"\\nPre-computing target CLIP IMAGE embeddings...\")\n",
        "        self._precompute_image_embeddings()\n",
        "\n",
        "        # Convert categorical columns to indices\n",
        "        if self.categorical_columns:\n",
        "            self.categorical_indices = torch.zeros((len(self.df), len(self.categorical_columns)), dtype=torch.long)\n",
        "            for i, col in enumerate(self.categorical_columns):\n",
        "                if col in self.df.columns:\n",
        "                    self.categorical_indices[:, i] = torch.tensor([\n",
        "                        self.cat_mappings[col][val] for val in self.df[col]\n",
        "                    ])\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"✓ Dataset ready with {len(self)} samples\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "    def _match_images_to_characters(self):\n",
        "        \"\"\"\n",
        "        Match character names from CSV to image filenames.\n",
        "        Handles spaces, underscores, and case-insensitive matching.\n",
        "        \"\"\"\n",
        "        # Get all image files in the directory\n",
        "        if not os.path.exists(self.image_dir):\n",
        "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n",
        "\n",
        "        image_files = [f for f in os.listdir(self.image_dir)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
        "\n",
        "        print(f\"Found {len(image_files)} image files in directory\")\n",
        "\n",
        "        # Create mapping of normalized names to actual filenames\n",
        "        image_map = {}\n",
        "        for img_file in image_files:\n",
        "            # Remove extension and normalize\n",
        "            name_without_ext = os.path.splitext(img_file)[0]\n",
        "            normalized = name_without_ext.lower().replace('_', ' ').replace('-', ' ')\n",
        "            image_map[normalized] = img_file\n",
        "\n",
        "        # Match each character to an image\n",
        "        self.df['image_path'] = None\n",
        "        matched_count = 0\n",
        "\n",
        "        for idx, row in self.df.iterrows():\n",
        "            char_name = str(row['character_name'])\n",
        "\n",
        "            # Try exact match first\n",
        "            if char_name in image_map:\n",
        "                self.df.at[idx, 'image_path'] = os.path.join(self.image_dir, image_map[char_name])\n",
        "                matched_count += 1\n",
        "                continue\n",
        "\n",
        "            # Try normalized matching\n",
        "            normalized_char = char_name.lower().replace('_', ' ').replace('-', ' ')\n",
        "            if normalized_char in image_map:\n",
        "                self.df.at[idx, 'image_path'] = os.path.join(self.image_dir, image_map[normalized_char])\n",
        "                matched_count += 1\n",
        "                print(f\"✓ Matched '{char_name}' → '{image_map[normalized_char]}'\")\n",
        "            else:\n",
        "                print(f\"✗ WARNING: No image found for '{char_name}'\")\n",
        "\n",
        "        print(f\"\\nMatched {matched_count}/{len(self.df)} characters to images\")\n",
        "\n",
        "        # Filter out rows without images\n",
        "        original_len = len(self.df)\n",
        "        self.df = self.df[self.df['image_path'].notna()].reset_index(drop=True)\n",
        "        if len(self.df) < original_len:\n",
        "            print(f\"Removed {original_len - len(self.df)} characters without images\")\n",
        "\n",
        "    def _precompute_text_embeddings(self):\n",
        "        \"\"\"Pre-compute CLIP text embeddings for character attributes\"\"\"\n",
        "        self.string_embeds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Text embeddings\"):\n",
        "                # Get string values for each column that exists\n",
        "                strings = []\n",
        "                for col in self.string_columns:\n",
        "                    if col in self.df.columns:\n",
        "                        strings.append(str(row[col]))\n",
        "\n",
        "                if strings:\n",
        "                    # Tokenize and encode\n",
        "                    tokens = clip.tokenize(strings, truncate=True).to(self.device)\n",
        "                    embeds = self.clip_model.encode_text(tokens)\n",
        "                    embeds = embeds / embeds.norm(dim=-1, keepdim=True)\n",
        "                    self.string_embeds.append(embeds.cpu())\n",
        "                else:\n",
        "                    # Fallback if no string columns\n",
        "                    self.string_embeds.append(torch.zeros(1, 512).cpu())\n",
        "\n",
        "    def _precompute_image_embeddings(self):\n",
        "        \"\"\"Pre-compute CLIP image embeddings from character images\"\"\"\n",
        "        self.target_image_embeds = []\n",
        "        failed_images = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Image embeddings\"):\n",
        "                img_path = row['image_path']\n",
        "\n",
        "                try:\n",
        "                    # Load and preprocess image\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "                    # Encode image with CLIP\n",
        "                    image_embed = self.clip_model.encode_image(image_input)\n",
        "                    image_embed = image_embed / image_embed.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                    self.target_image_embeds.append(image_embed.cpu())\n",
        "\n",
        "                except Exception as e:\n",
        "                    failed_images.append((row['character_name'], str(e)))\n",
        "                    # Use zero embedding as placeholder\n",
        "                    self.target_image_embeds.append(torch.zeros(1, 512).cpu())\n",
        "\n",
        "        if failed_images:\n",
        "            print(f\"\\n⚠ Failed to load {len(failed_images)} images:\")\n",
        "            for name, error in failed_images[:5]:  # Show first 5\n",
        "                print(f\"  - {name}: {error}\")\n",
        "\n",
        "    def _create_text_description(self, row):\n",
        "        \"\"\"Create a natural language description from row data\"\"\"\n",
        "        parts = [f\"{row['character_name']}\"]\n",
        "\n",
        "        if 'rarity' in row:\n",
        "            parts.append(f\"is a {row['rarity']}-star\")\n",
        "        if 'vision' in row:\n",
        "            parts.append(f\"{row['vision']} character\")\n",
        "        if 'region' in row:\n",
        "            parts.append(f\"from {row['region']}\")\n",
        "        if 'weapon_type' in row:\n",
        "            parts.append(f\"who wields a {row['weapon_type']}\")\n",
        "        if 'body_figure' in row:\n",
        "            parts.append(f\"with a {row['body_figure']} body figure\")\n",
        "        if 'affiliation' in row:\n",
        "            parts.append(f\"and belongs to {row['affiliation']}\")\n",
        "        if 'constellation' in row:\n",
        "            parts.append(f\"Their constellation is {row['constellation']}\")\n",
        "\n",
        "        return \" \".join(parts) + \".\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            string_embeds: (N, embed_dim) - CLIP embeddings for textual attributes\n",
        "            categorical_inputs: (M,) - indices for categorical inputs\n",
        "            base_text_embed: (1, embed_dim) - base text embedding\n",
        "            target_image_embed: (1, embed_dim) - target CLIP IMAGE embedding\n",
        "        \"\"\"\n",
        "        return (\n",
        "            self.string_embeds[idx],\n",
        "            self.categorical_indices[idx] if hasattr(self, 'categorical_indices') else torch.tensor([]),\n",
        "            self.base_text_embed,\n",
        "            self.target_image_embeds[idx]\n",
        "        )\n",
        "\n",
        "    def get_num_categories(self):\n",
        "        \"\"\"Returns number of categories for each categorical column\"\"\"\n",
        "        return [len(self.cat_mappings[col]) for col in self.categorical_columns if col in self.cat_mappings]\n",
        "\n",
        "    def get_raw_data(self, idx):\n",
        "        \"\"\"Get the original row data for inspection\"\"\"\n",
        "        return self.df.iloc[idx]\n",
        "\n",
        "    def get_character_name(self, idx):\n",
        "        \"\"\"Get character name for a given index\"\"\"\n",
        "        return self.df.iloc[idx]['character_name']\n",
        "\n",
        "    def get_image_path(self, idx):\n",
        "        \"\"\"Get image path for a given index\"\"\"\n",
        "        return self.df.iloc[idx]['image_path']\n",
        "\n",
        "\n",
        "def create_train_val_split(dataset, val_ratio=0.2, random_seed=42):\n",
        "    \"\"\"Split dataset into training and validation sets\"\"\"\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "\n",
        "    # Shuffle indices\n",
        "    torch.manual_seed(random_seed)\n",
        "    indices = torch.randperm(dataset_size).tolist()\n",
        "\n",
        "    # Split\n",
        "    split_idx = int(dataset_size * (1 - val_ratio))\n",
        "    train_indices = indices[:split_idx]\n",
        "    val_indices = indices[split_idx:]\n",
        "\n",
        "    # Create subset datasets\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load CLIP model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    print(\"Loading CLIP model...\")\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    clip_model.eval()\n",
        "    print(\"✓ CLIP model loaded\\n\")\n",
        "\n",
        "    # Create dataset\n",
        "    # The paths will use the defaults specified in __init__ if not provided\n",
        "    csv_file = \"/content/drive/MyDrive/filtered_output.csv\"\n",
        "    image_dir = \"/content/drive/MyDrive/characters/\"\n",
        "\n",
        "    dataset = CharacterDataset(\n",
        "        csv_file=csv_file,\n",
        "        clip_model=clip_model,\n",
        "        clip_preprocess=clip_preprocess,\n",
        "        device=device,\n",
        "        image_dir=image_dir\n",
        "    )\n",
        "\n",
        "    # Check number of categories\n",
        "    num_categories = dataset.get_num_categories()\n",
        "    print(f\"\\nNumber of categories per column: {num_categories}\")\n",
        "\n",
        "    # Split into train/val\n",
        "    train_dataset, val_dataset = create_train_val_split(dataset, val_ratio=0.2)\n",
        "    print(f\"\\nTrain size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Test loading a batch\n",
        "    print(\"\\nTesting batch loading...\")\n",
        "    for string_embeds, cat_inputs, base_embed, target_images in train_loader:\n",
        "        print(f\"\\nBatch shapes:\")\n",
        "        print(f\"  String embeddings: {string_embeds.shape}\")\n",
        "        print(f\"  Categorical inputs: {cat_inputs.shape}\")\n",
        "        print(f\"  Base text embedding: {base_embed.shape}\")\n",
        "        print(f\"  Target image embeddings: {target_images.shape}\")\n",
        "        break\n",
        "\n",
        "    # Inspect first few samples\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SAMPLE DATA INSPECTION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for i in range(min(3, len(dataset))):\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"  Character: {dataset.get_character_name(i)}\")\n",
        "        print(f\"  Image: {dataset.get_image_path(i)}\")\n",
        "        print(f\"  Raw data: {dataset.get_raw_data(i).to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0s2manc0DCs",
        "outputId": "081ecb72-d29a-42a1-b3aa-403843203e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cpu\n",
            "\n",
            "Loading CLIP model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 103MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ CLIP model loaded\n",
            "\n",
            "Loading CSV from: /content/drive/MyDrive/filtered_output.csv\n",
            "Looking for images in: /content/drive/MyDrive/characters/\n",
            "✓ Successfully loaded CSV with encoding: utf-8\n",
            "Loaded 73 characters from CSV\n",
            "Columns: ['character_name', 'region', 'vision', 'weapon_type', 'body_figure', 'rarity', 'constellation', 'affiliation']\n",
            "region: 6 unique values - ['Mondstadt' 'Sumeru' 'Inazuma' 'Liyue' 'Fontaine' 'Snezhnaya']\n",
            "vision: 7 unique values - ['Geo' 'Dendro' 'Pyro' 'Hydro' 'Electro' 'Cryo' 'Anemo']\n",
            "weapon_type: 5 unique values - ['Sword' 'Bow' 'Claymore' 'Catalyst' 'Polearm']\n",
            "body_figure: 5 unique values - ['Medium Male' 'Tall Male' 'Medium Female' 'Tall Female' 'Short Female']\n",
            "\n",
            "============================================================\n",
            "MATCHING CHARACTER NAMES TO IMAGE FILES\n",
            "============================================================\n",
            "Found 106 image files in directory\n",
            "✓ Matched 'Albedo' → 'Albedo.png'\n",
            "✓ Matched 'Alhaitham' → 'Alhaitham.png'\n",
            "✓ Matched 'Amber' → 'Amber.png'\n",
            "✓ Matched 'Arataki Itto' → 'Arataki_Itto.png'\n",
            "✓ Matched 'Baizhu' → 'Baizhu.png'\n",
            "✓ Matched 'Barbara' → 'Barbara.png'\n",
            "✓ Matched 'Beidou' → 'Beidou.png'\n",
            "✓ Matched 'Bennett' → 'Bennett.png'\n",
            "✓ Matched 'Candace' → 'Candace.png'\n",
            "✓ Matched 'Charlotte' → 'Charlotte.png'\n",
            "✓ Matched 'Chongyun' → 'Chongyun.png'\n",
            "✓ Matched 'Collei' → 'Collei.png'\n",
            "✓ Matched 'Cyno' → 'Cyno.png'\n",
            "✓ Matched 'Dehya' → 'Dehya.png'\n",
            "✓ Matched 'Diluc' → 'Diluc.png'\n",
            "✓ Matched 'Diona' → 'Diona.png'\n",
            "✓ Matched 'Dori' → 'Dori.png'\n",
            "✓ Matched 'Eula' → 'Eula.png'\n",
            "✓ Matched 'Faruzan' → 'Faruzan.png'\n",
            "✓ Matched 'Fischl' → 'Fischl.png'\n",
            "✓ Matched 'Freminet' → 'Freminet.png'\n",
            "✓ Matched 'Furina' → 'Furina.png'\n",
            "✓ Matched 'Ganyu' → 'Ganyu.png'\n",
            "✓ Matched 'Gorou' → 'Gorou.png'\n",
            "✓ Matched 'Hu Tao' → 'Hu_Tao.png'\n",
            "✓ Matched 'Jean' → 'Jean.png'\n",
            "✓ Matched 'Kaedehara Kazuha' → 'Kaedehara_Kazuha.png'\n",
            "✓ Matched 'Kaeya' → 'Kaeya.png'\n",
            "✓ Matched 'Kamisato Ayaka' → 'Kamisato_Ayaka.png'\n",
            "✓ Matched 'Kamisato Ayato' → 'Kamisato_Ayato.png'\n",
            "✓ Matched 'Kaveh' → 'Kaveh.png'\n",
            "✓ Matched 'Keqing' → 'Keqing.png'\n",
            "✓ Matched 'Kirara' → 'Kirara.png'\n",
            "✓ Matched 'Klee' → 'Klee.png'\n",
            "✓ Matched 'Kujou Sara' → 'Kujou_Sara.png'\n",
            "✓ Matched 'Kuki Shinobu' → 'Kuki_Shinobu.png'\n",
            "✓ Matched 'Layla' → 'Layla.png'\n",
            "✓ Matched 'Lisa' → 'Lisa.png'\n",
            "✓ Matched 'Lynette' → 'Lynette.png'\n",
            "✓ Matched 'Lyney' → 'Lyney.png'\n",
            "✓ Matched 'Mika' → 'Mika.png'\n",
            "✓ Matched 'Mona' → 'Mona.png'\n",
            "✓ Matched 'Nahida' → 'Nahida.png'\n",
            "✓ Matched 'Nilou' → 'Nilou.png'\n",
            "✓ Matched 'Ningguang' → 'Ningguang.png'\n",
            "✓ Matched 'Noelle' → 'Noelle.png'\n",
            "✓ Matched 'Neuvillette' → 'Neuvillette.png'\n",
            "✓ Matched 'Qiqi' → 'Qiqi.png'\n",
            "✓ Matched 'Raiden Shogun' → 'Raiden_Shogun.png'\n",
            "✓ Matched 'Razor' → 'Razor.png'\n",
            "✓ Matched 'Rosaria' → 'Rosaria.png'\n",
            "✓ Matched 'Sangonomiya Kokomi' → 'Sangonomiya_Kokomi.png'\n",
            "✓ Matched 'Sayu' → 'Sayu.png'\n",
            "✓ Matched 'Shenhe' → 'Shenhe.png'\n",
            "✓ Matched 'Shikanoin Heizou' → 'Shikanoin_Heizou.png'\n",
            "✓ Matched 'Sucrose' → 'Sucrose.png'\n",
            "✓ Matched 'Tartaglia' → 'Tartaglia.png'\n",
            "✓ Matched 'Thoma' → 'Thoma.png'\n",
            "✓ Matched 'Tighnari' → 'Tighnari.png'\n",
            "✓ Matched 'Venti' → 'Venti.png'\n",
            "✓ Matched 'Wanderer' → 'Wanderer.png'\n",
            "✓ Matched 'Wriothesley' → 'Wriothesley.png'\n",
            "✓ Matched 'Xiangling' → 'Xiangling.png'\n",
            "✓ Matched 'Xiao' → 'Xiao.png'\n",
            "✓ Matched 'Xingqiu' → 'Xingqiu.png'\n",
            "✓ Matched 'Xinyan' → 'Xinyan.png'\n",
            "✓ Matched 'Yae Miko' → 'Yae_Miko.png'\n",
            "✓ Matched 'Yanfei' → 'Yanfei.png'\n",
            "✓ Matched 'Yaoyao' → 'Yaoyao.png'\n",
            "✓ Matched 'Yelan' → 'Yelan.png'\n",
            "✓ Matched 'Yoimiya' → 'Yoimiya.png'\n",
            "✓ Matched 'Yun Jin' → 'Yun_Jin.png'\n",
            "✓ Matched 'Zhongli' → 'Zhongli.png'\n",
            "\n",
            "Matched 73/73 characters to images\n",
            "\n",
            "Pre-computing base text embedding for: 'Genshin-style character'\n",
            "\n",
            "Pre-computing CLIP TEXT embeddings for character attributes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Text embeddings: 100%|██████████| 73/73 [00:30<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-computing target CLIP IMAGE embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Image embeddings:  66%|██████▌   | 48/73 [00:35<00:18,  1.34it/s]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "Image embeddings: 100%|██████████| 73/73 [00:54<00:00,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "✓ Dataset ready with 73 samples\n",
            "============================================================\n",
            "\n",
            "\n",
            "Number of categories per column: [6, 7, 5, 5]\n",
            "\n",
            "Train size: 58, Val size: 15\n",
            "\n",
            "Testing batch loading...\n",
            "\n",
            "Batch shapes:\n",
            "  String embeddings: torch.Size([16, 3, 512])\n",
            "  Categorical inputs: torch.Size([16, 4])\n",
            "  Base text embedding: torch.Size([16, 1, 512])\n",
            "  Target image embeddings: torch.Size([16, 1, 512])\n",
            "\n",
            "============================================================\n",
            "SAMPLE DATA INSPECTION\n",
            "============================================================\n",
            "\n",
            "Sample 0:\n",
            "  Character: Albedo\n",
            "  Image: /content/drive/MyDrive/characters/Albedo.png\n",
            "  Raw data: {'character_name': 'Albedo', 'region': 'Mondstadt', 'vision': 'Geo', 'weapon_type': 'Sword', 'body_figure': 'Medium Male', 'rarity': 5, 'constellation': 'Princeps Cretaceus', 'affiliation': 'Knights of Favonius', 'image_path': '/content/drive/MyDrive/characters/Albedo.png'}\n",
            "\n",
            "Sample 1:\n",
            "  Character: Alhaitham\n",
            "  Image: /content/drive/MyDrive/characters/Alhaitham.png\n",
            "  Raw data: {'character_name': 'Alhaitham', 'region': 'Sumeru', 'vision': 'Dendro', 'weapon_type': 'Sword', 'body_figure': 'Tall Male', 'rarity': 5, 'constellation': 'Vultur Volans', 'affiliation': 'Sumeru Akademiya', 'image_path': '/content/drive/MyDrive/characters/Alhaitham.png'}\n",
            "\n",
            "Sample 2:\n",
            "  Character: Amber\n",
            "  Image: /content/drive/MyDrive/characters/Amber.png\n",
            "  Raw data: {'character_name': 'Amber', 'region': 'Mondstadt', 'vision': 'Pyro', 'weapon_type': 'Bow', 'body_figure': 'Medium Female', 'rarity': 4, 'constellation': 'Lepus', 'affiliation': 'Knights of Favonius', 'image_path': '/content/drive/MyDrive/characters/Amber.png'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CLIPOffsetMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP that predicts an offset in CLIP embedding space.\n",
        "    Architecture: concatenated [one-hot vectors, CLIP text embeddings] -> MLP -> offset vector\n",
        "    Final embedding: E_pred = E_base + E_offset\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        clip_dim=512,  # CLIP embedding dimension (512 for ViT-B/32, 768 for ViT-L/14)\n",
        "        string_embed_dim=512,  # dimension for string embeddings from CLIP\n",
        "        num_categories_per_attr=[6, 7, 5, 5],  # number of categories for each discrete attribute\n",
        "        num_text_attrs=3,  # number of textual attributes (character_name, constellation, affiliation)\n",
        "        hidden_dims=[1024, 1024, 512],  # hidden layer dimensions\n",
        "        normalize_inputs=True,  # normalize components before concatenation\n",
        "        dropout=0.1,  # dropout rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_dim = clip_dim\n",
        "        self.string_embed_dim = string_embed_dim\n",
        "        self.num_text_attrs = num_text_attrs\n",
        "        self.normalize_inputs = normalize_inputs\n",
        "\n",
        "        # Handle case where no categorical attributes are provided\n",
        "        if num_categories_per_attr is None or len(num_categories_per_attr) == 0:\n",
        "            self.num_categories_per_attr = []\n",
        "            total_onehot_dim = 0\n",
        "        else:\n",
        "            self.num_categories_per_attr = num_categories_per_attr\n",
        "            total_onehot_dim = sum(num_categories_per_attr)\n",
        "\n",
        "        # Calculate input dimensions\n",
        "        total_text_dim = num_text_attrs * string_embed_dim\n",
        "\n",
        "        # Total input dimension after concatenation\n",
        "        input_dim = total_onehot_dim + total_text_dim\n",
        "\n",
        "        print(f\"\\nModel Architecture:\")\n",
        "        print(f\"  Input dimensions:\")\n",
        "        print(f\"    - One-hot vectors: {total_onehot_dim}\")\n",
        "        print(f\"    - Text embeddings: {total_text_dim} ({num_text_attrs} × {string_embed_dim})\")\n",
        "        print(f\"    - Total input: {input_dim}\")\n",
        "        print(f\"  Hidden layers: {hidden_dims}\")\n",
        "        print(f\"  Output dimension: {clip_dim}\")\n",
        "        print(f\"  Normalize inputs: {normalize_inputs}\")\n",
        "        print(f\"  Dropout: {dropout}\\n\")\n",
        "\n",
        "        # Build MLP layers\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Final projection to CLIP dimension (offset vector)\n",
        "        layers.append(nn.Linear(prev_dim, clip_dim))\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, string_embeds, categorical_inputs, base_text_embed):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            string_embeds: tensor of shape (batch_size, num_text_attrs, string_embed_dim)\n",
        "                          Pre-computed CLIP embeddings for textual attributes\n",
        "            categorical_inputs: tensor of shape (batch_size, num_discrete_attrs) or empty tensor\n",
        "                               Integer indices for discrete attributes\n",
        "            base_text_embed: tensor of shape (batch_size, clip_dim) or (1, clip_dim)\n",
        "                            Base text embedding for \"Genshin-style character\"\n",
        "\n",
        "        Returns:\n",
        "            pred_embeddings: tensor of shape (batch_size, clip_dim)\n",
        "                            E_pred = E_base + E_offset\n",
        "        \"\"\"\n",
        "        batch_size = string_embeds.shape[0]\n",
        "\n",
        "        # 1. Process one-hot vectors for discrete attributes (if any)\n",
        "        if len(self.num_categories_per_attr) > 0 and categorical_inputs.numel() > 0:\n",
        "            onehot_vectors = []\n",
        "            for i, num_cats in enumerate(self.num_categories_per_attr):\n",
        "                onehot = F.one_hot(categorical_inputs[:, i].long(), num_classes=num_cats)\n",
        "                onehot_vectors.append(onehot.float())\n",
        "\n",
        "            x_onehot = torch.cat(onehot_vectors, dim=1)  # (batch_size, total_onehot_dim)\n",
        "        else:\n",
        "            x_onehot = None\n",
        "\n",
        "        # 2. Process text embeddings\n",
        "        # Shape: (batch_size, num_text_attrs, embed_dim) -> (batch_size, num_text_attrs * embed_dim)\n",
        "        x_text = string_embeds.reshape(batch_size, -1)\n",
        "\n",
        "        # 3. Normalize components before concatenation\n",
        "        if self.normalize_inputs:\n",
        "            # Normalize text embeddings (L2 norm)\n",
        "            x_text = F.normalize(x_text, p=2, dim=1)\n",
        "\n",
        "            # Normalize one-hot vector (L2 norm) if it exists\n",
        "            if x_onehot is not None:\n",
        "                x_onehot = F.normalize(x_onehot, p=2, dim=1)\n",
        "\n",
        "        # 4. Concatenate: x_input = [x_onehot, E_text_attr] or just E_text_attr\n",
        "        if x_onehot is not None:\n",
        "            x_input = torch.cat([x_onehot, x_text], dim=1)\n",
        "        else:\n",
        "            x_input = x_text\n",
        "\n",
        "        # 5. Pass through MLP to get offset vector\n",
        "        offset = self.mlp(x_input)\n",
        "\n",
        "        # 6. Add offset to base embedding: E_pred = E_base + E_offset\n",
        "        # Handle broadcasting if base_text_embed is (1, clip_dim)\n",
        "        if base_text_embed.dim() == 3:\n",
        "            # If base_text_embed is (batch_size, 1, clip_dim) or (1, 1, clip_dim)\n",
        "            base_text_embed = base_text_embed.squeeze(1)\n",
        "\n",
        "        if base_text_embed.shape[0] == 1 and batch_size > 1:\n",
        "            base_text_embed = base_text_embed.expand(batch_size, -1)\n",
        "\n",
        "        pred_embeddings = base_text_embed + offset\n",
        "\n",
        "        # 7. Normalize final embedding (CLIP embeddings are typically normalized)\n",
        "        pred_embeddings = F.normalize(pred_embeddings, p=2, dim=1)\n",
        "\n",
        "        return pred_embeddings\n",
        "\n",
        "    def inference(self, string_embeds, categorical_inputs, base_text_embed):\n",
        "        \"\"\"\n",
        "        Inference mode - identical to forward pass but explicitly named for clarity.\n",
        "\n",
        "        Args:\n",
        "            string_embeds: CLIP embeddings of textual attributes\n",
        "            categorical_inputs: Integer indices for discrete attributes\n",
        "            base_text_embed: Base embedding for \"Genshin-style character\"\n",
        "\n",
        "        Returns:\n",
        "            E_star: Conditioning vector for diffusion model\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.forward(string_embeds, categorical_inputs, base_text_embed)\n",
        "\n",
        "    def get_num_parameters(self):\n",
        "        \"\"\"Return the number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for string_embeds, cat_inputs, base_embed, target_images in dataloader:\n",
        "        # Move to device\n",
        "        string_embeds = string_embeds.to(device)\n",
        "        cat_inputs = cat_inputs.to(device)\n",
        "        base_embed = base_embed.to(device)\n",
        "        target_images = target_images.to(device)\n",
        "\n",
        "        # Squeeze target images if needed: (batch, 1, dim) -> (batch, dim)\n",
        "        if target_images.dim() == 3:\n",
        "            target_images = target_images.squeeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        pred_embeddings = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "        # Compute loss (cosine similarity loss)\n",
        "        loss = criterion(pred_embeddings, target_images)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for string_embeds, cat_inputs, base_embed, target_images in dataloader:\n",
        "            # Move to device\n",
        "            string_embeds = string_embeds.to(device)\n",
        "            cat_inputs = cat_inputs.to(device)\n",
        "            base_embed = base_embed.to(device)\n",
        "            target_images = target_images.to(device)\n",
        "\n",
        "            # Squeeze target images if needed\n",
        "            if target_images.dim() == 3:\n",
        "                target_images = target_images.squeeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_embeddings = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(pred_embeddings, target_images)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "# Cosine embedding loss (maximizes cosine similarity)\n",
        "class CosineSimilarityLoss(nn.Module):\n",
        "    \"\"\"Loss that maximizes cosine similarity between predicted and target embeddings\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Cosine similarity ranges from -1 to 1\n",
        "        # We want to maximize it, so minimize (1 - cosine_similarity)\n",
        "        cos_sim = F.cosine_similarity(pred, target, dim=1)\n",
        "        loss = 1 - cos_sim.mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "JEeEusUX2l8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After creating dataset:\n",
        "num_categories = dataset.get_num_categories()\n",
        "\n",
        "model = CLIPOffsetMLP(\n",
        "    clip_dim=512,  # ViT-B/32\n",
        "    string_embed_dim=512,\n",
        "    num_categories_per_attr=num_categories,\n",
        "    num_text_attrs=3,\n",
        "    hidden_dims=[1024, 1024, 512],\n",
        "    normalize_inputs=True,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Train\n",
        "criterion = CosineSimilarityLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLaVp0Mw5dLd",
        "outputId": "cf70c06f-c8d0-468f-acdb-9c2625242b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "  Input dimensions:\n",
            "    - One-hot vectors: 23\n",
            "    - Text embeddings: 1536 (3 × 512)\n",
            "    - Total input: 1559\n",
            "  Hidden layers: [1024, 1024, 512]\n",
            "  Output dimension: 512\n",
            "  Normalize inputs: True\n",
            "  Dropout: 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Cosine embedding loss (maximizes cosine similarity)\n",
        "class CosineSimilarityLoss(nn.Module):\n",
        "    \"\"\"Loss that maximizes cosine similarity between predicted and target embeddings\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Cosine similarity ranges from -1 to 1\n",
        "        # We want to maximize it, so minimize (1 - cosine_similarity)\n",
        "        cos_sim = F.cosine_similarity(pred, target, dim=1)\n",
        "        loss = 1 - cos_sim.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, grad_clip=1.0):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    cosine_similarities = []\n",
        "    num_batches = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "    for string_embeds, cat_inputs, base_embed, target_images in pbar:\n",
        "        # Move to device\n",
        "        string_embeds = string_embeds.to(device)\n",
        "        cat_inputs = cat_inputs.to(device)\n",
        "        base_embed = base_embed.to(device)\n",
        "        target_images = target_images.to(device)\n",
        "\n",
        "        # Squeeze target images if needed: (batch, 1, dim) -> (batch, dim)\n",
        "        if target_images.dim() == 3:\n",
        "            target_images = target_images.squeeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        pred_embeddings = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(pred_embeddings, target_images)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if grad_clip > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        with torch.no_grad():\n",
        "            cos_sim = F.cosine_similarity(pred_embeddings, target_images, dim=1)\n",
        "            cosine_similarities.extend(cos_sim.cpu().numpy())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_cos_sim = np.mean(cosine_similarities)\n",
        "\n",
        "    return avg_loss, avg_cos_sim\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    cosine_similarities = []\n",
        "    num_batches = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for string_embeds, cat_inputs, base_embed, target_images in pbar:\n",
        "            # Move to device\n",
        "            string_embeds = string_embeds.to(device)\n",
        "            cat_inputs = cat_inputs.to(device)\n",
        "            base_embed = base_embed.to(device)\n",
        "            target_images = target_images.to(device)\n",
        "\n",
        "            # Squeeze target images if needed\n",
        "            if target_images.dim() == 3:\n",
        "                target_images = target_images.squeeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_embeddings = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(pred_embeddings, target_images)\n",
        "\n",
        "            # Track metrics\n",
        "            cos_sim = F.cosine_similarity(pred_embeddings, target_images, dim=1)\n",
        "            cosine_similarities.extend(cos_sim.cpu().numpy())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_cos_sim = np.mean(cosine_similarities)\n",
        "\n",
        "    return avg_loss, avg_cos_sim\n",
        "\n",
        "\n",
        "def plot_training_curves(history, save_path='training_curves.png'):\n",
        "    \"\"\"Plot training and validation curves\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Loss curves\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Cosine similarity curves\n",
        "    ax2.plot(epochs, history['train_cos_sim'], 'b-', label='Train Cosine Sim', linewidth=2)\n",
        "    ax2.plot(epochs, history['val_cos_sim'], 'r-', label='Val Cosine Sim', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Cosine Similarity', fontsize=12)\n",
        "    ax2.set_title('Training and Validation Cosine Similarity', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved training curves to {save_path}\")\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, history, save_path):\n",
        "    \"\"\"Save a training checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'history': history,\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    return obj\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device):\n",
        "    \"\"\"Load a training checkpoint\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler and checkpoint['scheduler_state_dict']:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['history']\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    grad_clip=1.0,\n",
        "    device='cuda',\n",
        "    save_dir='./checkpoints',\n",
        "    early_stopping_patience=15,\n",
        "    save_every=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete training function with all the bells and whistles\n",
        "\n",
        "    Args:\n",
        "        model: The CLIPOffsetMLP model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Initial learning rate\n",
        "        weight_decay: Weight decay for AdamW\n",
        "        grad_clip: Gradient clipping threshold (0 to disable)\n",
        "        device: Device to train on\n",
        "        save_dir: Directory to save checkpoints and logs\n",
        "        early_stopping_patience: Stop if no improvement for N epochs\n",
        "        save_every: Save checkpoint every N epochs\n",
        "\n",
        "    Returns:\n",
        "        history: Dictionary with training history\n",
        "    \"\"\"\n",
        "    # Create save directory\n",
        "    save_dir = Path(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize training components\n",
        "    criterion = CosineSimilarityLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler - cosine annealing with warmup\n",
        "    warmup_epochs = 5\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs - warmup_epochs,\n",
        "        eta_min=learning_rate * 0.01\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_cos_sim': [],\n",
        "        'val_cos_sim': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING START\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Total epochs: {num_epochs}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(f\"Weight decay: {weight_decay}\")\n",
        "    print(f\"Gradient clipping: {grad_clip}\")\n",
        "    print(f\"Early stopping patience: {early_stopping_patience}\")\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Val batches: {len(val_loader)}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_cos_sim = train_epoch(\n",
        "            model, train_loader, optimizer, criterion, device, grad_clip\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_cos_sim = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update learning rate (after warmup)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        if epoch >= warmup_epochs:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_cos_sim'].append(train_cos_sim)\n",
        "        history['val_cos_sim'].append(val_cos_sim)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Cos Sim: {train_cos_sim:.4f}\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Cos Sim:   {val_cos_sim:.4f}\")\n",
        "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save best model weights only (lighter file)\n",
        "            best_model_path = save_dir / 'best_model_weights.pth'\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"✓ New best model! Saved weights to {best_model_path}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement ({patience_counter}/{early_stopping_patience})\")\n",
        "\n",
        "        # Save periodic checkpoint\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            checkpoint_path = save_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch + 1, history, checkpoint_path)\n",
        "            print(f\"✓ Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Early stopping triggered! No improvement for {early_stopping_patience} epochs.\")\n",
        "            print(f\"Best epoch was {best_epoch} with val loss: {best_val_loss:.4f}\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "            break\n",
        "\n",
        "    # Save final checkpoint\n",
        "    final_checkpoint_path = save_dir / 'final_checkpoint.pth'\n",
        "    save_checkpoint(model, optimizer, scheduler, epoch + 1, history, final_checkpoint_path)\n",
        "\n",
        "    # Save final model weights\n",
        "    final_weights_path = save_dir / 'final_model_weights.pth'\n",
        "    torch.save(model.state_dict(), final_weights_path)\n",
        "    print(f\"✓ Saved final model weights to {final_weights_path}\")\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_path = save_dir / 'training_curves.png'\n",
        "    plot_training_curves(history, save_path=plot_path)\n",
        "\n",
        "    # Save training history as JSON (convert numpy types to native Python)\n",
        "    history_path = save_dir / 'training_history.json'\n",
        "    serializable_history = convert_to_serializable(history)\n",
        "    with open(history_path, 'w') as f:\n",
        "        json.dump(serializable_history, f, indent=2)\n",
        "    print(f\"✓ Saved training history to {history_path}\")\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"Best val loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best val cosine similarity: {max(history['val_cos_sim']):.4f}\")\n",
        "    print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
        "    print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
        "    print(f\"Models saved in: {save_dir}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Example of how to train model with dataset\n",
        "    \"\"\"\n",
        "    import clip\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # Setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # Load CLIP model\n",
        "    print(\"Loading CLIP model...\")\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    clip_model.eval()\n",
        "    print(\"✓ CLIP model loaded\\n\")\n",
        "\n",
        "    # Create dataset\n",
        "    csv_file = \"/content/drive/MyDrive/filtered_output.csv\"\n",
        "    image_dir = \"/content/drive/MyDrive/characters/\"\n",
        "\n",
        "    dataset = CharacterDataset(\n",
        "        csv_file=csv_file,\n",
        "        clip_model=clip_model,\n",
        "        clip_preprocess=clip_preprocess,\n",
        "        device=device,\n",
        "        image_dir=image_dir\n",
        "    )\n",
        "\n",
        "    # Split into train/val\n",
        "    train_dataset, val_dataset = create_train_val_split(dataset, val_ratio=0.2)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Get number of categories from dataset\n",
        "    num_categories = dataset.get_num_categories()\n",
        "    print(f\"Number of categories per attribute: {num_categories}\\n\")\n",
        "\n",
        "    # Create model\n",
        "\n",
        "\n",
        "    model = CLIPOffsetMLP(\n",
        "        clip_dim=512,\n",
        "        string_embed_dim=512,\n",
        "        num_categories_per_attr=num_categories,\n",
        "        num_text_attrs=3,\n",
        "        hidden_dims=[1024, 1024, 512],\n",
        "        normalize_inputs=True,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model has {model.get_num_parameters():,} trainable parameters\\n\")\n",
        "\n",
        "    # Train the model\n",
        "    history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=100,\n",
        "        learning_rate=1e-4,\n",
        "        weight_decay=0.01,\n",
        "        grad_clip=1.0,\n",
        "        device=device,\n",
        "        save_dir='./checkpoints',\n",
        "        early_stopping_patience=15,\n",
        "        save_every=10\n",
        "    )\n",
        "\n",
        "    # Load best model for inference\n",
        "    model.load_state_dict(torch.load('./checkpoints/best_model_weights.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Model ready for inference!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgXttxw55d7G",
        "outputId": "7cf59d47-033a-4156-f562-9e661dd210ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Loading CLIP model...\n",
            "✓ CLIP model loaded\n",
            "\n",
            "Loading CSV from: /content/drive/MyDrive/filtered_output.csv\n",
            "Looking for images in: /content/drive/MyDrive/characters/\n",
            "✓ Successfully loaded CSV with encoding: utf-8\n",
            "Loaded 73 characters from CSV\n",
            "Columns: ['character_name', 'region', 'vision', 'weapon_type', 'body_figure', 'rarity', 'constellation', 'affiliation']\n",
            "region: 6 unique values - ['Mondstadt' 'Sumeru' 'Inazuma' 'Liyue' 'Fontaine' 'Snezhnaya']\n",
            "vision: 7 unique values - ['Geo' 'Dendro' 'Pyro' 'Hydro' 'Electro' 'Cryo' 'Anemo']\n",
            "weapon_type: 5 unique values - ['Sword' 'Bow' 'Claymore' 'Catalyst' 'Polearm']\n",
            "body_figure: 5 unique values - ['Medium Male' 'Tall Male' 'Medium Female' 'Tall Female' 'Short Female']\n",
            "\n",
            "============================================================\n",
            "MATCHING CHARACTER NAMES TO IMAGE FILES\n",
            "============================================================\n",
            "Found 106 image files in directory\n",
            "✓ Matched 'Albedo' → 'Albedo.png'\n",
            "✓ Matched 'Alhaitham' → 'Alhaitham.png'\n",
            "✓ Matched 'Amber' → 'Amber.png'\n",
            "✓ Matched 'Arataki Itto' → 'Arataki_Itto.png'\n",
            "✓ Matched 'Baizhu' → 'Baizhu.png'\n",
            "✓ Matched 'Barbara' → 'Barbara.png'\n",
            "✓ Matched 'Beidou' → 'Beidou.png'\n",
            "✓ Matched 'Bennett' → 'Bennett.png'\n",
            "✓ Matched 'Candace' → 'Candace.png'\n",
            "✓ Matched 'Charlotte' → 'Charlotte.png'\n",
            "✓ Matched 'Chongyun' → 'Chongyun.png'\n",
            "✓ Matched 'Collei' → 'Collei.png'\n",
            "✓ Matched 'Cyno' → 'Cyno.png'\n",
            "✓ Matched 'Dehya' → 'Dehya.png'\n",
            "✓ Matched 'Diluc' → 'Diluc.png'\n",
            "✓ Matched 'Diona' → 'Diona.png'\n",
            "✓ Matched 'Dori' → 'Dori.png'\n",
            "✓ Matched 'Eula' → 'Eula.png'\n",
            "✓ Matched 'Faruzan' → 'Faruzan.png'\n",
            "✓ Matched 'Fischl' → 'Fischl.png'\n",
            "✓ Matched 'Freminet' → 'Freminet.png'\n",
            "✓ Matched 'Furina' → 'Furina.png'\n",
            "✓ Matched 'Ganyu' → 'Ganyu.png'\n",
            "✓ Matched 'Gorou' → 'Gorou.png'\n",
            "✓ Matched 'Hu Tao' → 'Hu_Tao.png'\n",
            "✓ Matched 'Jean' → 'Jean.png'\n",
            "✓ Matched 'Kaedehara Kazuha' → 'Kaedehara_Kazuha.png'\n",
            "✓ Matched 'Kaeya' → 'Kaeya.png'\n",
            "✓ Matched 'Kamisato Ayaka' → 'Kamisato_Ayaka.png'\n",
            "✓ Matched 'Kamisato Ayato' → 'Kamisato_Ayato.png'\n",
            "✓ Matched 'Kaveh' → 'Kaveh.png'\n",
            "✓ Matched 'Keqing' → 'Keqing.png'\n",
            "✓ Matched 'Kirara' → 'Kirara.png'\n",
            "✓ Matched 'Klee' → 'Klee.png'\n",
            "✓ Matched 'Kujou Sara' → 'Kujou_Sara.png'\n",
            "✓ Matched 'Kuki Shinobu' → 'Kuki_Shinobu.png'\n",
            "✓ Matched 'Layla' → 'Layla.png'\n",
            "✓ Matched 'Lisa' → 'Lisa.png'\n",
            "✓ Matched 'Lynette' → 'Lynette.png'\n",
            "✓ Matched 'Lyney' → 'Lyney.png'\n",
            "✓ Matched 'Mika' → 'Mika.png'\n",
            "✓ Matched 'Mona' → 'Mona.png'\n",
            "✓ Matched 'Nahida' → 'Nahida.png'\n",
            "✓ Matched 'Nilou' → 'Nilou.png'\n",
            "✓ Matched 'Ningguang' → 'Ningguang.png'\n",
            "✓ Matched 'Noelle' → 'Noelle.png'\n",
            "✓ Matched 'Neuvillette' → 'Neuvillette.png'\n",
            "✓ Matched 'Qiqi' → 'Qiqi.png'\n",
            "✓ Matched 'Raiden Shogun' → 'Raiden_Shogun.png'\n",
            "✓ Matched 'Razor' → 'Razor.png'\n",
            "✓ Matched 'Rosaria' → 'Rosaria.png'\n",
            "✓ Matched 'Sangonomiya Kokomi' → 'Sangonomiya_Kokomi.png'\n",
            "✓ Matched 'Sayu' → 'Sayu.png'\n",
            "✓ Matched 'Shenhe' → 'Shenhe.png'\n",
            "✓ Matched 'Shikanoin Heizou' → 'Shikanoin_Heizou.png'\n",
            "✓ Matched 'Sucrose' → 'Sucrose.png'\n",
            "✓ Matched 'Tartaglia' → 'Tartaglia.png'\n",
            "✓ Matched 'Thoma' → 'Thoma.png'\n",
            "✓ Matched 'Tighnari' → 'Tighnari.png'\n",
            "✓ Matched 'Venti' → 'Venti.png'\n",
            "✓ Matched 'Wanderer' → 'Wanderer.png'\n",
            "✓ Matched 'Wriothesley' → 'Wriothesley.png'\n",
            "✓ Matched 'Xiangling' → 'Xiangling.png'\n",
            "✓ Matched 'Xiao' → 'Xiao.png'\n",
            "✓ Matched 'Xingqiu' → 'Xingqiu.png'\n",
            "✓ Matched 'Xinyan' → 'Xinyan.png'\n",
            "✓ Matched 'Yae Miko' → 'Yae_Miko.png'\n",
            "✓ Matched 'Yanfei' → 'Yanfei.png'\n",
            "✓ Matched 'Yaoyao' → 'Yaoyao.png'\n",
            "✓ Matched 'Yelan' → 'Yelan.png'\n",
            "✓ Matched 'Yoimiya' → 'Yoimiya.png'\n",
            "✓ Matched 'Yun Jin' → 'Yun_Jin.png'\n",
            "✓ Matched 'Zhongli' → 'Zhongli.png'\n",
            "\n",
            "Matched 73/73 characters to images\n",
            "\n",
            "Pre-computing base text embedding for: 'Genshin-style character'\n",
            "\n",
            "Pre-computing CLIP TEXT embeddings for character attributes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Text embeddings: 100%|██████████| 73/73 [00:33<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-computing target CLIP IMAGE embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Image embeddings:  66%|██████▌   | 48/73 [00:21<00:10,  2.34it/s]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "Image embeddings: 100%|██████████| 73/73 [00:30<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "✓ Dataset ready with 73 samples\n",
            "============================================================\n",
            "\n",
            "Number of categories per attribute: [6, 7, 5, 5]\n",
            "\n",
            "\n",
            "Model Architecture:\n",
            "  Input dimensions:\n",
            "    - One-hot vectors: 23\n",
            "    - Text embeddings: 1536 (3 × 512)\n",
            "    - Total input: 1559\n",
            "  Hidden layers: [1024, 1024, 512]\n",
            "  Output dimension: 512\n",
            "  Normalize inputs: True\n",
            "  Dropout: 0.1\n",
            "\n",
            "Model has 3,439,616 trainable parameters\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TRAINING START\n",
            "======================================================================\n",
            "Device: cpu\n",
            "Total epochs: 100\n",
            "Learning rate: 0.0001\n",
            "Weight decay: 0.01\n",
            "Gradient clipping: 1.0\n",
            "Early stopping patience: 15\n",
            "Train batches: 4\n",
            "Val batches: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9313 | Train Cos Sim: 0.0587\n",
            "Val Loss:   0.7425 | Val Cos Sim:   0.2575\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 2/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7050 | Train Cos Sim: 0.2879\n",
            "Val Loss:   0.5870 | Val Cos Sim:   0.4130\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 3/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5761 | Train Cos Sim: 0.4189\n",
            "Val Loss:   0.4944 | Val Cos Sim:   0.5056\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 4/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4867 | Train Cos Sim: 0.5105\n",
            "Val Loss:   0.4260 | Val Cos Sim:   0.5740\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 5/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4208 | Train Cos Sim: 0.5762\n",
            "Val Loss:   0.3710 | Val Cos Sim:   0.6290\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 6/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3701 | Train Cos Sim: 0.6280\n",
            "Val Loss:   0.3267 | Val Cos Sim:   0.6733\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 7/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3302 | Train Cos Sim: 0.6692\n",
            "Val Loss:   0.2896 | Val Cos Sim:   0.7104\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 8/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2928 | Train Cos Sim: 0.7046\n",
            "Val Loss:   0.2584 | Val Cos Sim:   0.7416\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 9/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2659 | Train Cos Sim: 0.7323\n",
            "Val Loss:   0.2329 | Val Cos Sim:   0.7671\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 10/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2462 | Train Cos Sim: 0.7519\n",
            "Val Loss:   0.2123 | Val Cos Sim:   0.7877\n",
            "Learning Rate: 0.000100\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "✓ Saved checkpoint to checkpoints/checkpoint_epoch_10.pth\n",
            "\n",
            "Epoch 11/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2240 | Train Cos Sim: 0.7739\n",
            "Val Loss:   0.1965 | Val Cos Sim:   0.8035\n",
            "Learning Rate: 0.000099\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 12/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2111 | Train Cos Sim: 0.7881\n",
            "Val Loss:   0.1845 | Val Cos Sim:   0.8155\n",
            "Learning Rate: 0.000099\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 13/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2027 | Train Cos Sim: 0.7969\n",
            "Val Loss:   0.1758 | Val Cos Sim:   0.8242\n",
            "Learning Rate: 0.000099\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 14/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1899 | Train Cos Sim: 0.8091\n",
            "Val Loss:   0.1695 | Val Cos Sim:   0.8305\n",
            "Learning Rate: 0.000098\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 15/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1836 | Train Cos Sim: 0.8151\n",
            "Val Loss:   0.1649 | Val Cos Sim:   0.8351\n",
            "Learning Rate: 0.000098\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 16/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1828 | Train Cos Sim: 0.8161\n",
            "Val Loss:   0.1616 | Val Cos Sim:   0.8384\n",
            "Learning Rate: 0.000097\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 17/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1800 | Train Cos Sim: 0.8200\n",
            "Val Loss:   0.1592 | Val Cos Sim:   0.8408\n",
            "Learning Rate: 0.000097\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 18/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1766 | Train Cos Sim: 0.8249\n",
            "Val Loss:   0.1574 | Val Cos Sim:   0.8426\n",
            "Learning Rate: 0.000096\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 19/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1755 | Train Cos Sim: 0.8241\n",
            "Val Loss:   0.1558 | Val Cos Sim:   0.8442\n",
            "Learning Rate: 0.000095\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 20/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1694 | Train Cos Sim: 0.8298\n",
            "Val Loss:   0.1547 | Val Cos Sim:   0.8453\n",
            "Learning Rate: 0.000095\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "✓ Saved checkpoint to checkpoints/checkpoint_epoch_20.pth\n",
            "\n",
            "Epoch 21/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1685 | Train Cos Sim: 0.8319\n",
            "Val Loss:   0.1536 | Val Cos Sim:   0.8464\n",
            "Learning Rate: 0.000094\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 22/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1681 | Train Cos Sim: 0.8320\n",
            "Val Loss:   0.1529 | Val Cos Sim:   0.8471\n",
            "Learning Rate: 0.000093\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 23/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1655 | Train Cos Sim: 0.8346\n",
            "Val Loss:   0.1524 | Val Cos Sim:   0.8476\n",
            "Learning Rate: 0.000092\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 24/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1609 | Train Cos Sim: 0.8381\n",
            "Val Loss:   0.1521 | Val Cos Sim:   0.8479\n",
            "Learning Rate: 0.000091\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 25/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1630 | Train Cos Sim: 0.8369\n",
            "Val Loss:   0.1519 | Val Cos Sim:   0.8481\n",
            "Learning Rate: 0.000091\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 26/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1621 | Train Cos Sim: 0.8380\n",
            "Val Loss:   0.1517 | Val Cos Sim:   0.8483\n",
            "Learning Rate: 0.000090\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 27/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1607 | Train Cos Sim: 0.8409\n",
            "Val Loss:   0.1515 | Val Cos Sim:   0.8485\n",
            "Learning Rate: 0.000089\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 28/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1580 | Train Cos Sim: 0.8408\n",
            "Val Loss:   0.1513 | Val Cos Sim:   0.8487\n",
            "Learning Rate: 0.000087\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 29/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1568 | Train Cos Sim: 0.8429\n",
            "Val Loss:   0.1513 | Val Cos Sim:   0.8487\n",
            "Learning Rate: 0.000086\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 30/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1553 | Train Cos Sim: 0.8450\n",
            "Val Loss:   0.1513 | Val Cos Sim:   0.8487\n",
            "Learning Rate: 0.000085\n",
            "No improvement (1/15)\n",
            "✓ Saved checkpoint to checkpoints/checkpoint_epoch_30.pth\n",
            "\n",
            "Epoch 31/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1526 | Train Cos Sim: 0.8475\n",
            "Val Loss:   0.1512 | Val Cos Sim:   0.8488\n",
            "Learning Rate: 0.000084\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 32/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1494 | Train Cos Sim: 0.8497\n",
            "Val Loss:   0.1510 | Val Cos Sim:   0.8490\n",
            "Learning Rate: 0.000083\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 33/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1514 | Train Cos Sim: 0.8481\n",
            "Val Loss:   0.1508 | Val Cos Sim:   0.8492\n",
            "Learning Rate: 0.000082\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 34/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1508 | Train Cos Sim: 0.8488\n",
            "Val Loss:   0.1506 | Val Cos Sim:   0.8494\n",
            "Learning Rate: 0.000080\n",
            "✓ New best model! Saved weights to checkpoints/best_model_weights.pth\n",
            "\n",
            "Epoch 35/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1462 | Train Cos Sim: 0.8527\n",
            "Val Loss:   0.1507 | Val Cos Sim:   0.8493\n",
            "Learning Rate: 0.000079\n",
            "No improvement (1/15)\n",
            "\n",
            "Epoch 36/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1478 | Train Cos Sim: 0.8524\n",
            "Val Loss:   0.1507 | Val Cos Sim:   0.8493\n",
            "Learning Rate: 0.000078\n",
            "No improvement (2/15)\n",
            "\n",
            "Epoch 37/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1488 | Train Cos Sim: 0.8522\n",
            "Val Loss:   0.1509 | Val Cos Sim:   0.8491\n",
            "Learning Rate: 0.000076\n",
            "No improvement (3/15)\n",
            "\n",
            "Epoch 38/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1462 | Train Cos Sim: 0.8545\n",
            "Val Loss:   0.1511 | Val Cos Sim:   0.8489\n",
            "Learning Rate: 0.000075\n",
            "No improvement (4/15)\n",
            "\n",
            "Epoch 39/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1427 | Train Cos Sim: 0.8568\n",
            "Val Loss:   0.1512 | Val Cos Sim:   0.8488\n",
            "Learning Rate: 0.000073\n",
            "No improvement (5/15)\n",
            "\n",
            "Epoch 40/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1426 | Train Cos Sim: 0.8578\n",
            "Val Loss:   0.1512 | Val Cos Sim:   0.8488\n",
            "Learning Rate: 0.000072\n",
            "No improvement (6/15)\n",
            "✓ Saved checkpoint to checkpoints/checkpoint_epoch_40.pth\n",
            "\n",
            "Epoch 41/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1418 | Train Cos Sim: 0.8574\n",
            "Val Loss:   0.1512 | Val Cos Sim:   0.8488\n",
            "Learning Rate: 0.000070\n",
            "No improvement (7/15)\n",
            "\n",
            "Epoch 42/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1411 | Train Cos Sim: 0.8586\n",
            "Val Loss:   0.1514 | Val Cos Sim:   0.8486\n",
            "Learning Rate: 0.000069\n",
            "No improvement (8/15)\n",
            "\n",
            "Epoch 43/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1431 | Train Cos Sim: 0.8580\n",
            "Val Loss:   0.1514 | Val Cos Sim:   0.8486\n",
            "Learning Rate: 0.000067\n",
            "No improvement (9/15)\n",
            "\n",
            "Epoch 44/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1396 | Train Cos Sim: 0.8613\n",
            "Val Loss:   0.1515 | Val Cos Sim:   0.8485\n",
            "Learning Rate: 0.000066\n",
            "No improvement (10/15)\n",
            "\n",
            "Epoch 45/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1389 | Train Cos Sim: 0.8616\n",
            "Val Loss:   0.1516 | Val Cos Sim:   0.8484\n",
            "Learning Rate: 0.000064\n",
            "No improvement (11/15)\n",
            "\n",
            "Epoch 46/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1379 | Train Cos Sim: 0.8622\n",
            "Val Loss:   0.1518 | Val Cos Sim:   0.8482\n",
            "Learning Rate: 0.000063\n",
            "No improvement (12/15)\n",
            "\n",
            "Epoch 47/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1363 | Train Cos Sim: 0.8644\n",
            "Val Loss:   0.1521 | Val Cos Sim:   0.8479\n",
            "Learning Rate: 0.000061\n",
            "No improvement (13/15)\n",
            "\n",
            "Epoch 48/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1379 | Train Cos Sim: 0.8624\n",
            "Val Loss:   0.1523 | Val Cos Sim:   0.8477\n",
            "Learning Rate: 0.000059\n",
            "No improvement (14/15)\n",
            "\n",
            "Epoch 49/100\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1337 | Train Cos Sim: 0.8661\n",
            "Val Loss:   0.1525 | Val Cos Sim:   0.8475\n",
            "Learning Rate: 0.000058\n",
            "No improvement (15/15)\n",
            "\n",
            "======================================================================\n",
            "Early stopping triggered! No improvement for 15 epochs.\n",
            "Best epoch was 34 with val loss: 0.1506\n",
            "======================================================================\n",
            "\n",
            "✓ Saved final model weights to checkpoints/final_model_weights.pth\n",
            "✓ Saved training curves to checkpoints/training_curves.png\n",
            "✓ Saved training history to checkpoints/training_history.json\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE\n",
            "======================================================================\n",
            "Best epoch: 34\n",
            "Best val loss: 0.1506\n",
            "Best val cosine similarity: 0.8494\n",
            "Final train loss: 0.1337\n",
            "Final val loss: 0.1525\n",
            "Models saved in: checkpoints\n",
            "======================================================================\n",
            "\n",
            "Model ready for inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cosine_similarity_loss(predictions, targets):\n",
        "    \"\"\"\n",
        "    Loss based on cosine similarity between predicted and target embeddings.\n",
        "    L = 1 - cos(E_pred, E_img)\n",
        "    \"\"\"\n",
        "    # targets has shape (batch_size, 1, embed_dim), squeeze it\n",
        "    targets = targets.squeeze(1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = F.cosine_similarity(predictions, targets, dim=1)\n",
        "\n",
        "    # Convert to loss (1 = no similarity, 0 = perfect similarity)\n",
        "    loss = 1 - cosine_sim\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def train_clip_mlp(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=50,\n",
        "    lr=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    device='cuda',\n",
        "    save_path='best_clip_offset_mlp.pth',\n",
        "    patience=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the CLIP offset MLP using cosine similarity loss.\n",
        "    Model predicts: E_pred = E_base + MLP(x_input)\n",
        "    Loss: L = 1 - cos(E_pred, E_img)\n",
        "\n",
        "    Args:\n",
        "        model: CLIPOffsetMLP model instance\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data\n",
        "        num_epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        weight_decay: Weight decay for AdamW optimizer\n",
        "        device: Device to train on ('cuda' or 'cpu')\n",
        "        save_path: Path to save the best model\n",
        "        patience: Early stopping patience (epochs without improvement)\n",
        "\n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        history: Dictionary containing training history\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_cos_sim': [],\n",
        "        'val_loss': [],\n",
        "        'val_cos_sim': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Starting training on {device}...\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ============= TRAINING =============\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_cos_sim = 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for string_embeds, cat_inputs, base_text_embed, target_images in train_pbar:\n",
        "            # Move to device\n",
        "            string_embeds = string_embeds.to(device)\n",
        "            cat_inputs = cat_inputs.to(device)\n",
        "            base_text_embed = base_text_embed.to(device)\n",
        "            target_images = target_images.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: E_pred = E_base + MLP(x_input)\n",
        "            predictions = model(string_embeds, cat_inputs, base_text_embed)\n",
        "\n",
        "            # Compute loss: L = 1 - cos(E_pred, E_img)\n",
        "            loss = cosine_similarity_loss(predictions, target_images)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (optional but helpful)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                cos_sim = F.cosine_similarity(predictions, target_images.squeeze(1), dim=1)\n",
        "                train_cos_sim += cos_sim.mean().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'cos_sim': f'{cos_sim.mean().item():.4f}'\n",
        "            })\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_cos_sim /= len(train_loader)\n",
        "\n",
        "        # ============= VALIDATION =============\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_cos_sim = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]  \")\n",
        "            for string_embeds, cat_inputs, base_text_embed, target_images in val_pbar:\n",
        "                # Move to device\n",
        "                string_embeds = string_embeds.to(device)\n",
        "                cat_inputs = cat_inputs.to(device)\n",
        "                base_text_embed = base_text_embed.to(device)\n",
        "                target_images = target_images.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                predictions = model(string_embeds, cat_inputs, base_text_embed)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = cosine_similarity_loss(predictions, target_images)\n",
        "\n",
        "                # Track metrics\n",
        "                val_loss += loss.item()\n",
        "                cos_sim = F.cosine_similarity(predictions, target_images.squeeze(1), dim=1)\n",
        "                val_cos_sim += cos_sim.mean().item()\n",
        "\n",
        "                # Update progress bar\n",
        "                val_pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'cos_sim': f'{cos_sim.mean().item():.4f}'\n",
        "                })\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_cos_sim /= len(val_loader)\n",
        "\n",
        "        # Update learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step()\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_cos_sim'].append(train_cos_sim)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_cos_sim'].append(val_cos_sim)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train CosSim: {train_cos_sim:.4f}\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f} | Val CosSim:   {val_cos_sim:.4f}\")\n",
        "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_cos_sim': val_cos_sim,\n",
        "            }, save_path)\n",
        "            print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f}, val_cos_sim: {val_cos_sim:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement for {patience_counter} epoch(s)\")\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    print(f\"\\nLoading best model from {save_path}\")\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Best model - Epoch: {checkpoint['epoch']+1}, Val Loss: {checkpoint['val_loss']:.4f}, Val CosSim: {checkpoint['val_cos_sim']:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='o')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Cosine similarity plot\n",
        "    axes[1].plot(history['train_cos_sim'], label='Train CosSim', marker='o')\n",
        "    axes[1].plot(history['val_cos_sim'], label='Val CosSim', marker='o')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Cosine Similarity')\n",
        "    axes[1].set_title('Training and Validation Cosine Similarity')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Learning rate plot\n",
        "    axes[2].plot(history['lr'], marker='o', color='green')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Learning Rate')\n",
        "    axes[2].set_title('Learning Rate Schedule')\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============= COMPLETE EXAMPLE USAGE =============\n",
        "if __name__ == \"__main__\":\n",
        "    import clip\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # Setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # Load CLIP model\n",
        "    print(\"Loading CLIP model...\")\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    clip_model.eval()\n",
        "\n",
        "    # Load dataset (using the CharacterDataset from previous artifact)\n",
        "    print(\"\\nLoading dataset from CSV...\")\n",
        "    csv_file = \"characters.csv\"\n",
        "    image_dir = \"character_images/\"\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing CLIP Offset MLP model...\")\n",
        "\n",
        "    # Get number of categories from dataset\n",
        "    num_categories = dataset.get_num_categories()  # e.g., [7, 7, 5, 3]\n",
        "\n",
        "    model = CLIPOffsetMLP(\n",
        "        clip_dim=512,  # ViT-B/32 uses 512, ViT-L/14 uses 768\n",
        "        string_embed_dim=512,\n",
        "        num_categories_per_attr=num_categories,\n",
        "        hidden_dims=[1024, 1024, 512],\n",
        "        normalize_inputs=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nStarting training...\\n\")\n",
        "    trained_model, history = train_clip_mlp(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=50,\n",
        "        lr=1e-4,\n",
        "        weight_decay=0.01,\n",
        "        device=device,\n",
        "        save_path='best_clip_offset_mlp.pth',\n",
        "        patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    print(\"\\nPlotting training history...\")\n",
        "    plot_training_history(history)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Best model saved to: best_clip_offset_mlp.pth\")\n",
        "\n",
        "    # Example inference\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE EXAMPLE\")\n",
        "    print(\"=\"*80)\n",
        "    trained_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get first sample from validation set\n",
        "        string_embeds, cat_inputs, base_embed, _ = next(iter(val_loader))\n",
        "\n",
        "        # Move to device and take first sample\n",
        "        string_embeds = string_embeds[:1].to(device)\n",
        "        cat_inputs = cat_inputs[:1].to(device)\n",
        "        base_embed = base_embed[:1].to(device)\n",
        "\n",
        "        # Inference: E* = E_text + E_offset\n",
        "        embedding_star = trained_model.inference(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "        print(f\"Generated embedding shape: {embedding_star.shape}\")\n",
        "        print(f\"Embedding norm: {embedding_star.norm().item():.4f}\")"
      ],
      "metadata": {
        "id": "NW2Pc7lluww5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61d15653-711d-4c55-e3ff-c3317a59d015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Loading CLIP model...\n",
            "\n",
            "Loading dataset from CSV...\n",
            "\n",
            "Initializing CLIP Offset MLP model...\n",
            "\n",
            "Model Architecture:\n",
            "  Input dimensions:\n",
            "    - One-hot vectors: 23\n",
            "    - Text embeddings: 1536 (3 × 512)\n",
            "    - Total input: 1559\n",
            "  Hidden layers: [1024, 1024, 512]\n",
            "  Output dimension: 512\n",
            "  Normalize inputs: True\n",
            "  Dropout: 0.1\n",
            "\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Starting training on cpu...\n",
            "Model parameters: 3,439,616\n",
            "Trainable parameters: 3,439,616\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50 [Train]: 100%|██████████| 4/4 [00:01<00:00,  3.89it/s, loss=0.8118, cos_sim=0.1882]\n",
            "Epoch 1/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.57it/s, loss=0.7223, cos_sim=0.2777]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50 Summary:\n",
            "  Train Loss: 0.8940 | Train CosSim: 0.1060\n",
            "  Val Loss:   0.7223 | Val CosSim:   0.2777\n",
            "  Learning Rate: 0.000100\n",
            "  ✓ Saved best model (val_loss: 0.7223, val_cos_sim: 0.2777)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  4.61it/s, loss=0.6291, cos_sim=0.3709]\n",
            "Epoch 2/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.80it/s, loss=0.5608, cos_sim=0.4392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/50 Summary:\n",
            "  Train Loss: 0.6767 | Train CosSim: 0.3233\n",
            "  Val Loss:   0.5608 | Val CosSim:   0.4392\n",
            "  Learning Rate: 0.000100\n",
            "  ✓ Saved best model (val_loss: 0.5608, val_cos_sim: 0.4392)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  4.52it/s, loss=0.5281, cos_sim=0.4719]\n",
            "Epoch 3/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  2.02it/s, loss=0.4631, cos_sim=0.5369]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/50 Summary:\n",
            "  Train Loss: 0.5495 | Train CosSim: 0.4505\n",
            "  Val Loss:   0.4631 | Val CosSim:   0.5369\n",
            "  Learning Rate: 0.000100\n",
            "  ✓ Saved best model (val_loss: 0.4631, val_cos_sim: 0.5369)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 [Train]: 100%|██████████| 4/4 [00:01<00:00,  3.54it/s, loss=0.4232, cos_sim=0.5768]\n",
            "Epoch 4/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  1.89it/s, loss=0.3933, cos_sim=0.6067]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/50 Summary:\n",
            "  Train Loss: 0.4577 | Train CosSim: 0.5423\n",
            "  Val Loss:   0.3933 | Val CosSim:   0.6067\n",
            "  Learning Rate: 0.000099\n",
            "  ✓ Saved best model (val_loss: 0.3933, val_cos_sim: 0.6067)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  4.17it/s, loss=0.3788, cos_sim=0.6212]\n",
            "Epoch 5/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  2.20it/s, loss=0.3396, cos_sim=0.6604]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/50 Summary:\n",
            "  Train Loss: 0.3945 | Train CosSim: 0.6055\n",
            "  Val Loss:   0.3396 | Val CosSim:   0.6604\n",
            "  Learning Rate: 0.000098\n",
            "  ✓ Saved best model (val_loss: 0.3396, val_cos_sim: 0.6604)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  5.93it/s, loss=0.3405, cos_sim=0.6595]\n",
            "Epoch 6/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.32it/s, loss=0.2970, cos_sim=0.7030]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/50 Summary:\n",
            "  Train Loss: 0.3494 | Train CosSim: 0.6506\n",
            "  Val Loss:   0.2970 | Val CosSim:   0.7030\n",
            "  Learning Rate: 0.000098\n",
            "  ✓ Saved best model (val_loss: 0.2970, val_cos_sim: 0.7030)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  5.70it/s, loss=0.2965, cos_sim=0.7035]\n",
            "Epoch 7/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.38it/s, loss=0.2627, cos_sim=0.7373]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/50 Summary:\n",
            "  Train Loss: 0.3073 | Train CosSim: 0.6927\n",
            "  Val Loss:   0.2627 | Val CosSim:   0.7373\n",
            "  Learning Rate: 0.000096\n",
            "  ✓ Saved best model (val_loss: 0.2627, val_cos_sim: 0.7373)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.36it/s, loss=0.2617, cos_sim=0.7383]\n",
            "Epoch 8/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.39it/s, loss=0.2347, cos_sim=0.7653]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/50 Summary:\n",
            "  Train Loss: 0.2788 | Train CosSim: 0.7212\n",
            "  Val Loss:   0.2347 | Val CosSim:   0.7653\n",
            "  Learning Rate: 0.000095\n",
            "  ✓ Saved best model (val_loss: 0.2347, val_cos_sim: 0.7653)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.12it/s, loss=0.2410, cos_sim=0.7590]\n",
            "Epoch 9/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.56it/s, loss=0.2129, cos_sim=0.7871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/50 Summary:\n",
            "  Train Loss: 0.2519 | Train CosSim: 0.7481\n",
            "  Val Loss:   0.2129 | Val CosSim:   0.7871\n",
            "  Learning Rate: 0.000094\n",
            "  ✓ Saved best model (val_loss: 0.2129, val_cos_sim: 0.7871)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.49it/s, loss=0.2235, cos_sim=0.7765]\n",
            "Epoch 10/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.66it/s, loss=0.1965, cos_sim=0.8035]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/50 Summary:\n",
            "  Train Loss: 0.2323 | Train CosSim: 0.7677\n",
            "  Val Loss:   0.1965 | Val CosSim:   0.8035\n",
            "  Learning Rate: 0.000092\n",
            "  ✓ Saved best model (val_loss: 0.1965, val_cos_sim: 0.8035)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.53it/s, loss=0.2174, cos_sim=0.7826]\n",
            "Epoch 11/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.43it/s, loss=0.1845, cos_sim=0.8155]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/50 Summary:\n",
            "  Train Loss: 0.2198 | Train CosSim: 0.7802\n",
            "  Val Loss:   0.1845 | Val CosSim:   0.8155\n",
            "  Learning Rate: 0.000090\n",
            "  ✓ Saved best model (val_loss: 0.1845, val_cos_sim: 0.8155)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.20it/s, loss=0.2066, cos_sim=0.7934]\n",
            "Epoch 12/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.88it/s, loss=0.1759, cos_sim=0.8241]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/50 Summary:\n",
            "  Train Loss: 0.2087 | Train CosSim: 0.7913\n",
            "  Val Loss:   0.1759 | Val CosSim:   0.8241\n",
            "  Learning Rate: 0.000089\n",
            "  ✓ Saved best model (val_loss: 0.1759, val_cos_sim: 0.8241)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.46it/s, loss=0.1996, cos_sim=0.8004]\n",
            "Epoch 13/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, loss=0.1697, cos_sim=0.8303]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/50 Summary:\n",
            "  Train Loss: 0.1963 | Train CosSim: 0.8037\n",
            "  Val Loss:   0.1697 | Val CosSim:   0.8303\n",
            "  Learning Rate: 0.000086\n",
            "  ✓ Saved best model (val_loss: 0.1697, val_cos_sim: 0.8303)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.45it/s, loss=0.1873, cos_sim=0.8127]\n",
            "Epoch 14/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.56it/s, loss=0.1652, cos_sim=0.8348]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/50 Summary:\n",
            "  Train Loss: 0.1936 | Train CosSim: 0.8064\n",
            "  Val Loss:   0.1652 | Val CosSim:   0.8348\n",
            "  Learning Rate: 0.000084\n",
            "  ✓ Saved best model (val_loss: 0.1652, val_cos_sim: 0.8348)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.91it/s, loss=0.2003, cos_sim=0.7997]\n",
            "Epoch 15/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.60it/s, loss=0.1618, cos_sim=0.8382]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/50 Summary:\n",
            "  Train Loss: 0.1848 | Train CosSim: 0.8152\n",
            "  Val Loss:   0.1618 | Val CosSim:   0.8382\n",
            "  Learning Rate: 0.000082\n",
            "  ✓ Saved best model (val_loss: 0.1618, val_cos_sim: 0.8382)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.31it/s, loss=0.1753, cos_sim=0.8247]\n",
            "Epoch 16/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.47it/s, loss=0.1592, cos_sim=0.8408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/50 Summary:\n",
            "  Train Loss: 0.1810 | Train CosSim: 0.8190\n",
            "  Val Loss:   0.1592 | Val CosSim:   0.8408\n",
            "  Learning Rate: 0.000079\n",
            "  ✓ Saved best model (val_loss: 0.1592, val_cos_sim: 0.8408)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.05it/s, loss=0.1892, cos_sim=0.8108]\n",
            "Epoch 17/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.53it/s, loss=0.1575, cos_sim=0.8425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/50 Summary:\n",
            "  Train Loss: 0.1780 | Train CosSim: 0.8220\n",
            "  Val Loss:   0.1575 | Val CosSim:   0.8425\n",
            "  Learning Rate: 0.000077\n",
            "  ✓ Saved best model (val_loss: 0.1575, val_cos_sim: 0.8425)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.14it/s, loss=0.1829, cos_sim=0.8171]\n",
            "Epoch 18/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.16it/s, loss=0.1560, cos_sim=0.8440]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/50 Summary:\n",
            "  Train Loss: 0.1755 | Train CosSim: 0.8245\n",
            "  Val Loss:   0.1560 | Val CosSim:   0.8440\n",
            "  Learning Rate: 0.000074\n",
            "  ✓ Saved best model (val_loss: 0.1560, val_cos_sim: 0.8440)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.70it/s, loss=0.1580, cos_sim=0.8420]\n",
            "Epoch 19/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.35it/s, loss=0.1549, cos_sim=0.8451]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/50 Summary:\n",
            "  Train Loss: 0.1701 | Train CosSim: 0.8299\n",
            "  Val Loss:   0.1549 | Val CosSim:   0.8451\n",
            "  Learning Rate: 0.000071\n",
            "  ✓ Saved best model (val_loss: 0.1549, val_cos_sim: 0.8451)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  7.35it/s, loss=0.1733, cos_sim=0.8267]\n",
            "Epoch 20/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.77it/s, loss=0.1542, cos_sim=0.8458]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/50 Summary:\n",
            "  Train Loss: 0.1731 | Train CosSim: 0.8269\n",
            "  Val Loss:   0.1542 | Val CosSim:   0.8458\n",
            "  Learning Rate: 0.000068\n",
            "  ✓ Saved best model (val_loss: 0.1542, val_cos_sim: 0.8458)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.05it/s, loss=0.1866, cos_sim=0.8134]\n",
            "Epoch 21/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.98it/s, loss=0.1536, cos_sim=0.8464]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/50 Summary:\n",
            "  Train Loss: 0.1732 | Train CosSim: 0.8268\n",
            "  Val Loss:   0.1536 | Val CosSim:   0.8464\n",
            "  Learning Rate: 0.000065\n",
            "  ✓ Saved best model (val_loss: 0.1536, val_cos_sim: 0.8464)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.35it/s, loss=0.1621, cos_sim=0.8379]\n",
            "Epoch 22/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.83it/s, loss=0.1533, cos_sim=0.8467]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/50 Summary:\n",
            "  Train Loss: 0.1688 | Train CosSim: 0.8312\n",
            "  Val Loss:   0.1533 | Val CosSim:   0.8467\n",
            "  Learning Rate: 0.000062\n",
            "  ✓ Saved best model (val_loss: 0.1533, val_cos_sim: 0.8467)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.05it/s, loss=0.1578, cos_sim=0.8422]\n",
            "Epoch 23/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.73it/s, loss=0.1529, cos_sim=0.8471]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/50 Summary:\n",
            "  Train Loss: 0.1660 | Train CosSim: 0.8340\n",
            "  Val Loss:   0.1529 | Val CosSim:   0.8471\n",
            "  Learning Rate: 0.000059\n",
            "  ✓ Saved best model (val_loss: 0.1529, val_cos_sim: 0.8471)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.98it/s, loss=0.1559, cos_sim=0.8441]\n",
            "Epoch 24/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, loss=0.1525, cos_sim=0.8475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/50 Summary:\n",
            "  Train Loss: 0.1640 | Train CosSim: 0.8360\n",
            "  Val Loss:   0.1525 | Val CosSim:   0.8475\n",
            "  Learning Rate: 0.000056\n",
            "  ✓ Saved best model (val_loss: 0.1525, val_cos_sim: 0.8475)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.53it/s, loss=0.1584, cos_sim=0.8416]\n",
            "Epoch 25/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.67it/s, loss=0.1522, cos_sim=0.8478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/50 Summary:\n",
            "  Train Loss: 0.1663 | Train CosSim: 0.8337\n",
            "  Val Loss:   0.1522 | Val CosSim:   0.8478\n",
            "  Learning Rate: 0.000053\n",
            "  ✓ Saved best model (val_loss: 0.1522, val_cos_sim: 0.8478)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.13it/s, loss=0.1553, cos_sim=0.8447]\n",
            "Epoch 26/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.51it/s, loss=0.1521, cos_sim=0.8479]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/50 Summary:\n",
            "  Train Loss: 0.1632 | Train CosSim: 0.8368\n",
            "  Val Loss:   0.1521 | Val CosSim:   0.8479\n",
            "  Learning Rate: 0.000050\n",
            "  ✓ Saved best model (val_loss: 0.1521, val_cos_sim: 0.8479)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.96it/s, loss=0.1597, cos_sim=0.8403]\n",
            "Epoch 27/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.42it/s, loss=0.1519, cos_sim=0.8481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/50 Summary:\n",
            "  Train Loss: 0.1638 | Train CosSim: 0.8362\n",
            "  Val Loss:   0.1519 | Val CosSim:   0.8481\n",
            "  Learning Rate: 0.000047\n",
            "  ✓ Saved best model (val_loss: 0.1519, val_cos_sim: 0.8481)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.32it/s, loss=0.1624, cos_sim=0.8376]\n",
            "Epoch 28/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.83it/s, loss=0.1518, cos_sim=0.8482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/50 Summary:\n",
            "  Train Loss: 0.1603 | Train CosSim: 0.8397\n",
            "  Val Loss:   0.1518 | Val CosSim:   0.8482\n",
            "  Learning Rate: 0.000044\n",
            "  ✓ Saved best model (val_loss: 0.1518, val_cos_sim: 0.8482)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.79it/s, loss=0.1530, cos_sim=0.8470]\n",
            "Epoch 29/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.19it/s, loss=0.1516, cos_sim=0.8484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/50 Summary:\n",
            "  Train Loss: 0.1610 | Train CosSim: 0.8390\n",
            "  Val Loss:   0.1516 | Val CosSim:   0.8484\n",
            "  Learning Rate: 0.000041\n",
            "  ✓ Saved best model (val_loss: 0.1516, val_cos_sim: 0.8484)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.56it/s, loss=0.1713, cos_sim=0.8287]\n",
            "Epoch 30/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.45it/s, loss=0.1514, cos_sim=0.8486]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/50 Summary:\n",
            "  Train Loss: 0.1633 | Train CosSim: 0.8367\n",
            "  Val Loss:   0.1514 | Val CosSim:   0.8486\n",
            "  Learning Rate: 0.000038\n",
            "  ✓ Saved best model (val_loss: 0.1514, val_cos_sim: 0.8486)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.91it/s, loss=0.1621, cos_sim=0.8379]\n",
            "Epoch 31/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.17it/s, loss=0.1512, cos_sim=0.8488]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31/50 Summary:\n",
            "  Train Loss: 0.1619 | Train CosSim: 0.8381\n",
            "  Val Loss:   0.1512 | Val CosSim:   0.8488\n",
            "  Learning Rate: 0.000035\n",
            "  ✓ Saved best model (val_loss: 0.1512, val_cos_sim: 0.8488)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  7.78it/s, loss=0.1532, cos_sim=0.8468]\n",
            "Epoch 32/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.29it/s, loss=0.1511, cos_sim=0.8489]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32/50 Summary:\n",
            "  Train Loss: 0.1602 | Train CosSim: 0.8398\n",
            "  Val Loss:   0.1511 | Val CosSim:   0.8489\n",
            "  Learning Rate: 0.000032\n",
            "  ✓ Saved best model (val_loss: 0.1511, val_cos_sim: 0.8489)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.26it/s, loss=0.1650, cos_sim=0.8350]\n",
            "Epoch 33/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.38it/s, loss=0.1510, cos_sim=0.8490]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33/50 Summary:\n",
            "  Train Loss: 0.1596 | Train CosSim: 0.8404\n",
            "  Val Loss:   0.1510 | Val CosSim:   0.8490\n",
            "  Learning Rate: 0.000029\n",
            "  ✓ Saved best model (val_loss: 0.1510, val_cos_sim: 0.8490)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  8.34it/s, loss=0.1538, cos_sim=0.8462]\n",
            "Epoch 34/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.53it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34/50 Summary:\n",
            "  Train Loss: 0.1599 | Train CosSim: 0.8401\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000026\n",
            "  ✓ Saved best model (val_loss: 0.1509, val_cos_sim: 0.8491)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.79it/s, loss=0.1651, cos_sim=0.8349]\n",
            "Epoch 35/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.04it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35/50 Summary:\n",
            "  Train Loss: 0.1592 | Train CosSim: 0.8408\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000023\n",
            "  ✓ Saved best model (val_loss: 0.1509, val_cos_sim: 0.8491)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.11it/s, loss=0.1507, cos_sim=0.8493]\n",
            "Epoch 36/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.51it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36/50 Summary:\n",
            "  Train Loss: 0.1592 | Train CosSim: 0.8408\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000021\n",
            "  ✓ Saved best model (val_loss: 0.1509, val_cos_sim: 0.8491)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.60it/s, loss=0.1576, cos_sim=0.8424]\n",
            "Epoch 37/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.54it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37/50 Summary:\n",
            "  Train Loss: 0.1587 | Train CosSim: 0.8413\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000018\n",
            "  No improvement for 1 epoch(s)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.67it/s, loss=0.1597, cos_sim=0.8403]\n",
            "Epoch 38/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.54it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38/50 Summary:\n",
            "  Train Loss: 0.1577 | Train CosSim: 0.8423\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000016\n",
            "  No improvement for 2 epoch(s)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.03it/s, loss=0.1453, cos_sim=0.8547]\n",
            "Epoch 39/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.27it/s, loss=0.1509, cos_sim=0.8491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39/50 Summary:\n",
            "  Train Loss: 0.1541 | Train CosSim: 0.8459\n",
            "  Val Loss:   0.1509 | Val CosSim:   0.8491\n",
            "  Learning Rate: 0.000014\n",
            "  No improvement for 3 epoch(s)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  7.37it/s, loss=0.1453, cos_sim=0.8547]\n",
            "Epoch 40/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.88it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40/50 Summary:\n",
            "  Train Loss: 0.1542 | Train CosSim: 0.8458\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000011\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.98it/s, loss=0.1481, cos_sim=0.8519]\n",
            "Epoch 41/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.06it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 41/50 Summary:\n",
            "  Train Loss: 0.1557 | Train CosSim: 0.8443\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000010\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  7.25it/s, loss=0.1538, cos_sim=0.8462]\n",
            "Epoch 42/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 42/50 Summary:\n",
            "  Train Loss: 0.1560 | Train CosSim: 0.8440\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000008\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s, loss=0.1533, cos_sim=0.8467]\n",
            "Epoch 43/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.23it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 43/50 Summary:\n",
            "  Train Loss: 0.1559 | Train CosSim: 0.8441\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000006\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.36it/s, loss=0.1456, cos_sim=0.8544]\n",
            "Epoch 44/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.44it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 44/50 Summary:\n",
            "  Train Loss: 0.1537 | Train CosSim: 0.8463\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000005\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.76it/s, loss=0.1498, cos_sim=0.8502]\n",
            "Epoch 45/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.48it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 45/50 Summary:\n",
            "  Train Loss: 0.1572 | Train CosSim: 0.8428\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000004\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.87it/s, loss=0.1458, cos_sim=0.8542]\n",
            "Epoch 46/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 46/50 Summary:\n",
            "  Train Loss: 0.1544 | Train CosSim: 0.8456\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000002\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 10.04it/s, loss=0.1533, cos_sim=0.8467]\n",
            "Epoch 47/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  6.48it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 47/50 Summary:\n",
            "  Train Loss: 0.1555 | Train CosSim: 0.8445\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000002\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  9.45it/s, loss=0.1569, cos_sim=0.8431]\n",
            "Epoch 48/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  4.93it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 48/50 Summary:\n",
            "  Train Loss: 0.1577 | Train CosSim: 0.8423\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000001\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  6.31it/s, loss=0.1558, cos_sim=0.8442]\n",
            "Epoch 49/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  5.00it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 49/50 Summary:\n",
            "  Train Loss: 0.1543 | Train CosSim: 0.8457\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000000\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50 [Train]: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s, loss=0.1576, cos_sim=0.8424]\n",
            "Epoch 50/50 [Val]  : 100%|██████████| 1/1 [00:00<00:00,  3.22it/s, loss=0.1508, cos_sim=0.8492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 50/50 Summary:\n",
            "  Train Loss: 0.1580 | Train CosSim: 0.8420\n",
            "  Val Loss:   0.1508 | Val CosSim:   0.8492\n",
            "  Learning Rate: 0.000000\n",
            "  ✓ Saved best model (val_loss: 0.1508, val_cos_sim: 0.8492)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Loading best model from best_clip_offset_mlp.pth\n",
            "Best model - Epoch: 50, Val Loss: 0.1508, Val CosSim: 0.8492\n",
            "\n",
            "Plotting training history...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete!\n",
            "Best model saved to: best_clip_offset_mlp.pth\n",
            "\n",
            "================================================================================\n",
            "INFERENCE EXAMPLE\n",
            "================================================================================\n",
            "Generated embedding shape: torch.Size([1, 512])\n",
            "Embedding norm: 1.0000\n",
            "\n",
            "This embedding can now be used as conditioning for your diffusion model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model_path, device='cpu'):  # Changed default to 'cpu'\n",
        "    \"\"\"Quick inference test\"\"\"\n",
        "    # Load model\n",
        "    model = CLIPOffsetMLP(\n",
        "        clip_dim=512,\n",
        "        string_embed_dim=512,\n",
        "        num_categories_per_attr=[6, 7, 5, 5],\n",
        "        hidden_dims=[1024, 1024, 512]\n",
        "    )\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy inputs\n",
        "    batch_size = 2\n",
        "    string_embeds = torch.randn(batch_size, 3, 512).to(device)\n",
        "    categorical_inputs = torch.tensor([[0, 1, 2, 3], [5, 6, 4, 2]]).to(device)\n",
        "    base_text_embed = torch.randn(1, 512).to(device)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(string_embeds, categorical_inputs, base_text_embed)\n",
        "\n",
        "    print(f\"✓ Inference successful!\")\n",
        "    print(f\"  Output shape: {output.shape}\")\n",
        "    print(f\"  Output norm: {output.norm(dim=1).mean():.4f}\")\n",
        "    return output\n",
        "\n",
        "# Run test\n",
        "output = test_inference('best_clip_offset_mlp.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT5H4zwV7mP2",
        "outputId": "f90bd7b1-fb3c-4f39-af02-3f83ab879f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "  Input dimensions:\n",
            "    - One-hot vectors: 23\n",
            "    - Text embeddings: 1536 (3 × 512)\n",
            "    - Total input: 1559\n",
            "  Hidden layers: [1024, 1024, 512]\n",
            "  Output dimension: 512\n",
            "  Normalize inputs: True\n",
            "  Dropout: 0.1\n",
            "\n",
            "✓ Inference successful!\n",
            "  Output shape: torch.Size([2, 512])\n",
            "  Output norm: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import clip\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def compute_similarity_metrics(model, dataset, device, num_samples=None):\n",
        "    \"\"\"\n",
        "    Compute detailed similarity metrics between predicted and target embeddings\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with per-sample cosine similarities and character names\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if num_samples is None:\n",
        "        num_samples = len(dataset)\n",
        "\n",
        "    results = {\n",
        "        'character_names': [],\n",
        "        'cosine_similarities': [],\n",
        "        'predicted_embeddings': [],\n",
        "        'target_embeddings': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(min(num_samples, len(dataset))):\n",
        "            # Get data\n",
        "            string_embeds, cat_inputs, base_embed, target_image = dataset[idx]\n",
        "\n",
        "            # Add batch dimension\n",
        "            string_embeds = string_embeds.unsqueeze(0).to(device)\n",
        "            cat_inputs = cat_inputs.unsqueeze(0).to(device)\n",
        "            base_embed = base_embed.to(device)\n",
        "            target_image = target_image.to(device)\n",
        "\n",
        "            # Squeeze if needed\n",
        "            if target_image.dim() == 3:\n",
        "                target_image = target_image.squeeze(1)\n",
        "\n",
        "            # Predict\n",
        "            pred_embedding = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            cos_sim = F.cosine_similarity(pred_embedding, target_image, dim=1)\n",
        "\n",
        "            # Store results\n",
        "            results['character_names'].append(dataset.dataset.get_character_name(dataset.indices[idx]))\n",
        "            results['cosine_similarities'].append(cos_sim.item())\n",
        "            results['predicted_embeddings'].append(pred_embedding.cpu().numpy())\n",
        "            results['target_embeddings'].append(target_image.cpu().numpy())\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_similarity_distribution(results, save_path='similarity_distribution.png'):\n",
        "    \"\"\"Plot distribution of cosine similarities\"\"\"\n",
        "    similarities = results['cosine_similarities']\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Histogram\n",
        "    ax1.hist(similarities, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    ax1.axvline(np.mean(similarities), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(similarities):.3f}')\n",
        "    ax1.axvline(np.median(similarities), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(similarities):.3f}')\n",
        "    ax1.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.set_title('Distribution of Cosine Similarities', fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Box plot\n",
        "    ax2.boxplot(similarities, vert=True)\n",
        "    ax2.set_ylabel('Cosine Similarity', fontsize=12)\n",
        "    ax2.set_title('Cosine Similarity Box Plot', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved similarity distribution to {save_path}\")\n",
        "\n",
        "\n",
        "def plot_top_and_worst(results, n=5, save_path='top_worst_predictions.png'):\n",
        "    \"\"\"Show top N and worst N predictions\"\"\"\n",
        "    similarities = np.array(results['cosine_similarities'])\n",
        "    names = results['character_names']\n",
        "\n",
        "    # Get indices\n",
        "    top_indices = np.argsort(similarities)[-n:][::-1]\n",
        "    worst_indices = np.argsort(similarities)[:n]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    # Top predictions\n",
        "    top_names = [names[i] for i in top_indices]\n",
        "    top_sims = [similarities[i] for i in top_indices]\n",
        "    axes[0].barh(range(n), top_sims, color='green', alpha=0.7)\n",
        "    axes[0].set_yticks(range(n))\n",
        "    axes[0].set_yticklabels(top_names)\n",
        "    axes[0].set_xlabel('Cosine Similarity', fontsize=12)\n",
        "    axes[0].set_title(f'Top {n} Predictions', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlim([0, 1])\n",
        "    for i, v in enumerate(top_sims):\n",
        "        axes[0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
        "\n",
        "    # Worst predictions\n",
        "    worst_names = [names[i] for i in worst_indices]\n",
        "    worst_sims = [similarities[i] for i in worst_indices]\n",
        "    axes[1].barh(range(n), worst_sims, color='red', alpha=0.7)\n",
        "    axes[1].set_yticks(range(n))\n",
        "    axes[1].set_yticklabels(worst_names)\n",
        "    axes[1].set_xlabel('Cosine Similarity', fontsize=12)\n",
        "    axes[1].set_title(f'Worst {n} Predictions', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlim([0, 1])\n",
        "    for i, v in enumerate(worst_sims):\n",
        "        axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved top/worst predictions to {save_path}\")\n",
        "\n",
        "\n",
        "def visualize_embeddings_2d(results, method='pca', save_path='embeddings_2d.png'):\n",
        "    \"\"\"\n",
        "    Visualize embeddings in 2D using PCA or t-SNE\n",
        "\n",
        "    Args:\n",
        "        results: Results dictionary from compute_similarity_metrics\n",
        "        method: 'pca' or 'tsne'\n",
        "        save_path: Path to save the plot\n",
        "    \"\"\"\n",
        "    # Stack embeddings\n",
        "    pred_embeds = np.vstack(results['predicted_embeddings'])\n",
        "    target_embeds = np.vstack(results['target_embeddings'])\n",
        "\n",
        "    # Combine for dimensionality reduction\n",
        "    all_embeds = np.vstack([pred_embeds, target_embeds])\n",
        "\n",
        "    # Reduce dimensionality\n",
        "    if method == 'pca':\n",
        "        reducer = PCA(n_components=2, random_state=42)\n",
        "        embeds_2d = reducer.fit_transform(all_embeds)\n",
        "        title_suffix = f'(Explained Var: {sum(reducer.explained_variance_ratio_):.1%})'\n",
        "    else:  # tsne\n",
        "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeds)//2))\n",
        "        embeds_2d = reducer.fit_transform(all_embeds)\n",
        "        title_suffix = ''\n",
        "\n",
        "    # Split back\n",
        "    n_samples = len(pred_embeds)\n",
        "    pred_2d = embeds_2d[:n_samples]\n",
        "    target_2d = embeds_2d[n_samples:]\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # Plot targets (actual images)\n",
        "    scatter1 = ax.scatter(target_2d[:, 0], target_2d[:, 1],\n",
        "                         c='blue', s=100, alpha=0.6, marker='o',\n",
        "                         label='Target (Real Images)', edgecolors='black', linewidths=1)\n",
        "\n",
        "    # Plot predictions\n",
        "    scatter2 = ax.scatter(pred_2d[:, 0], pred_2d[:, 1],\n",
        "                         c='red', s=100, alpha=0.6, marker='^',\n",
        "                         label='Predicted (MLP Output)', edgecolors='black', linewidths=1)\n",
        "\n",
        "    # Draw lines connecting corresponding points\n",
        "    for i in range(n_samples):\n",
        "        ax.plot([pred_2d[i, 0], target_2d[i, 0]],\n",
        "               [pred_2d[i, 1], target_2d[i, 1]],\n",
        "               'gray', alpha=0.3, linewidth=0.5)\n",
        "\n",
        "    # Annotate some points\n",
        "    n_annotate = min(10, n_samples)\n",
        "    for i in range(n_annotate):\n",
        "        ax.annotate(results['character_names'][i],\n",
        "                   (target_2d[i, 0], target_2d[i, 1]),\n",
        "                   fontsize=8, alpha=0.7)\n",
        "\n",
        "    ax.set_xlabel(f'Component 1', fontsize=12)\n",
        "    ax.set_ylabel(f'Component 2', fontsize=12)\n",
        "    ax.set_title(f'CLIP Embedding Space Visualization ({method.upper()}) {title_suffix}',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved 2D embedding visualization to {save_path}\")\n",
        "\n",
        "\n",
        "def test_single_character(model, dataset, idx, clip_model, device, k=5):\n",
        "    \"\"\"\n",
        "    Test a single character and find its nearest neighbors in CLIP space\n",
        "\n",
        "    Args:\n",
        "        model: Trained MLP model\n",
        "        dataset: Dataset object\n",
        "        idx: Index of character to test\n",
        "        clip_model: CLIP model for computing image similarities\n",
        "        device: torch device\n",
        "        k: Number of nearest neighbors to find\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get character info\n",
        "    if hasattr(dataset, 'indices'):\n",
        "        actual_idx = dataset.indices[idx]\n",
        "        char_name = dataset.dataset.get_character_name(actual_idx)\n",
        "        char_data = dataset.dataset.get_raw_data(actual_idx)\n",
        "        img_path = dataset.dataset.get_image_path(actual_idx)\n",
        "    else:\n",
        "        char_name = dataset.get_character_name(idx)\n",
        "        char_data = dataset.get_raw_data(idx)\n",
        "        img_path = dataset.get_image_path(idx)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"TESTING CHARACTER: {char_name}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Image path: {img_path}\")\n",
        "    print(f\"\\nCharacter attributes:\")\n",
        "    for key, value in char_data.items():\n",
        "        if key != 'image_path':\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Get data\n",
        "    string_embeds, cat_inputs, base_embed, target_image = dataset[idx]\n",
        "\n",
        "    # Add batch dimension\n",
        "    string_embeds = string_embeds.unsqueeze(0).to(device)\n",
        "    cat_inputs = cat_inputs.unsqueeze(0).to(device)\n",
        "    base_embed = base_embed.to(device)\n",
        "    target_image = target_image.to(device)\n",
        "\n",
        "    if target_image.dim() == 3:\n",
        "        target_image = target_image.squeeze(1)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        pred_embedding = model(string_embeds, cat_inputs, base_embed)\n",
        "\n",
        "    # Compute similarity\n",
        "    cos_sim = F.cosine_similarity(pred_embedding, target_image, dim=1)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"RESULTS:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Cosine Similarity: {cos_sim.item():.4f}\")\n",
        "    print(f\"Embedding Distance (L2): {torch.dist(pred_embedding, target_image).item():.4f}\")\n",
        "\n",
        "    # Find nearest neighbors in the dataset\n",
        "    print(f\"\\nFinding {k} nearest neighbors to PREDICTED embedding...\")\n",
        "    all_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            _, _, _, other_target = dataset[i]\n",
        "            other_target = other_target.to(device)\n",
        "            if other_target.dim() == 3:\n",
        "                other_target = other_target.squeeze(1)\n",
        "\n",
        "            sim = F.cosine_similarity(pred_embedding, other_target, dim=1)\n",
        "            all_similarities.append(sim.item())\n",
        "\n",
        "    # Get top k\n",
        "    top_k_indices = np.argsort(all_similarities)[-k:][::-1]\n",
        "\n",
        "    print(f\"\\nTop {k} most similar characters (based on predicted embedding):\")\n",
        "    for rank, i in enumerate(top_k_indices, 1):\n",
        "        if hasattr(dataset, 'indices'):\n",
        "            actual_i = dataset.indices[i]\n",
        "            name = dataset.dataset.get_character_name(actual_i)\n",
        "        else:\n",
        "            name = dataset.get_character_name(i)\n",
        "        print(f\"  {rank}. {name}: {all_similarities[i]:.4f}\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'character_name': char_name,\n",
        "        'cosine_similarity': cos_sim.item(),\n",
        "        'predicted_embedding': pred_embedding.cpu().numpy(),\n",
        "        'target_embedding': target_image.cpu().numpy(),\n",
        "        'nearest_neighbors': [(dataset.dataset.get_character_name(dataset.indices[i]) if hasattr(dataset, 'indices') else dataset.get_character_name(i),\n",
        "                              all_similarities[i]) for i in top_k_indices]\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_text_to_image_retrieval(model, dataset, clip_model, device,\n",
        "                                     test_descriptions, k=5):\n",
        "    \"\"\"\n",
        "    Test text-to-image retrieval: given a text description, find most similar images\n",
        "\n",
        "    Args:\n",
        "        model: Trained MLP model\n",
        "        dataset: Dataset object\n",
        "        clip_model: CLIP model\n",
        "        device: torch device\n",
        "        test_descriptions: List of text descriptions to test\n",
        "        k: Number of top results to return\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get all target image embeddings from dataset\n",
        "    all_targets = []\n",
        "    all_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            _, _, _, target = dataset[i]\n",
        "            all_targets.append(target.squeeze())\n",
        "            if hasattr(dataset, 'indices'):\n",
        "                all_names.append(dataset.dataset.get_character_name(dataset.indices[i]))\n",
        "            else:\n",
        "                all_names.append(dataset.get_character_name(i))\n",
        "\n",
        "    all_targets = torch.stack(all_targets).to(device)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TEXT-TO-IMAGE RETRIEVAL TEST\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for desc in test_descriptions:\n",
        "        print(f\"\\nQuery: '{desc}'\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Encode text with CLIP\n",
        "        with torch.no_grad():\n",
        "            text_tokens = clip.tokenize([desc]).to(device)\n",
        "            text_embed = clip_model.encode_text(text_tokens)\n",
        "            text_embed = text_embed / text_embed.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute similarities with all images\n",
        "        similarities = F.cosine_similarity(text_embed, all_targets, dim=1)\n",
        "\n",
        "        # Get top k\n",
        "        top_k_indices = torch.argsort(similarities, descending=True)[:k]\n",
        "\n",
        "        print(f\"Top {k} matches:\")\n",
        "        for rank, idx in enumerate(top_k_indices, 1):\n",
        "            print(f\"  {rank}. {all_names[idx]}: {similarities[idx].item():.4f}\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "def comprehensive_evaluation(model, val_dataset, clip_model, device, save_dir='./evaluation'):\n",
        "    \"\"\"\n",
        "    Run comprehensive evaluation and generate all visualizations\n",
        "    \"\"\"\n",
        "    save_dir = Path(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # 1. Compute similarity metrics\n",
        "    print(\"Computing similarity metrics...\")\n",
        "    results = compute_similarity_metrics(model, val_dataset, device)\n",
        "\n",
        "    # Print summary statistics\n",
        "    sims = results['cosine_similarities']\n",
        "    print(f\"\\nSummary Statistics:\")\n",
        "    print(f\"  Mean cosine similarity: {np.mean(sims):.4f}\")\n",
        "    print(f\"  Median cosine similarity: {np.median(sims):.4f}\")\n",
        "    print(f\"  Std deviation: {np.std(sims):.4f}\")\n",
        "    print(f\"  Min: {np.min(sims):.4f}\")\n",
        "    print(f\"  Max: {np.max(sims):.4f}\")\n",
        "\n",
        "    # 2. Plot similarity distribution\n",
        "    print(\"\\nGenerating similarity distribution plot...\")\n",
        "    plot_similarity_distribution(results, save_path=save_dir / 'similarity_distribution.png')\n",
        "\n",
        "    # 3. Plot top and worst predictions\n",
        "    print(\"Generating top/worst predictions plot...\")\n",
        "    plot_top_and_worst(results, n=5, save_path=save_dir / 'top_worst_predictions.png')\n",
        "\n",
        "    # 4. Visualize embeddings in 2D (PCA)\n",
        "    print(\"Generating PCA visualization...\")\n",
        "    visualize_embeddings_2d(results, method='pca', save_path=save_dir / 'embeddings_pca.png')\n",
        "\n",
        "    # 5. Visualize embeddings in 2D (t-SNE)\n",
        "    print(\"Generating t-SNE visualization...\")\n",
        "    visualize_embeddings_2d(results, method='tsne', save_path=save_dir / 'embeddings_tsne.png')\n",
        "\n",
        "    # 6. Test a few random characters\n",
        "    print(\"\\nTesting random characters...\")\n",
        "    num_tests = min(3, len(val_dataset))\n",
        "    for i in range(num_tests):\n",
        "        idx = np.random.randint(0, len(val_dataset))\n",
        "        test_single_character(model, val_dataset, idx, clip_model, device, k=5)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"✓ Evaluation complete! Results saved to {save_dir}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Example\n",
        "    \"\"\"\n",
        "    import clip\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load CLIP\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    clip_model.eval()\n",
        "\n",
        "\n",
        "    # Initialize model architecture (must match training)\n",
        "    model = CLIPOffsetMLP(\n",
        "        clip_dim=512,\n",
        "        string_embed_dim=512,\n",
        "        num_categories_per_attr=[6, 7, 5, 5],\n",
        "        num_text_attrs=3,\n",
        "        hidden_dims=[1024, 1024, 512],\n",
        "        normalize_inputs=True,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Load trained weights\n",
        "    model.load_state_dict(torch.load('./checkpoints/best_model_weights.pth'))\n",
        "    model.eval()\n",
        "    # Run comprehensive evaluation\n",
        "    results = comprehensive_evaluation(model, val_dataset, clip_model, device, save_dir='./evaluation')\n",
        "\n",
        "    # Test specific character\n",
        "    test_single_character(model, val_dataset, idx=0, clip_model=clip_model, device=device, k=5)\n",
        "\n",
        "    # Test text-to-image retrieval\n",
        "    test_descriptions = [\n",
        "        \"A pyro character from Liyue\",\n",
        "        \"An electro polearm user\",\n",
        "        \"A character from Mondstadt\"\n",
        "    ]\n",
        "    generate_text_to_image_retrieval(model, val_dataset, clip_model, device, test_descriptions, k=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXX-rX_i29Gs",
        "outputId": "461bcc2e-83ae-46bf-c069-9656b95278a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "  Input dimensions:\n",
            "    - One-hot vectors: 23\n",
            "    - Text embeddings: 1536 (3 × 512)\n",
            "    - Total input: 1559\n",
            "  Hidden layers: [1024, 1024, 512]\n",
            "  Output dimension: 512\n",
            "  Normalize inputs: True\n",
            "  Dropout: 0.1\n",
            "\n",
            "\n",
            "======================================================================\n",
            "COMPREHENSIVE MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Computing similarity metrics...\n",
            "\n",
            "Summary Statistics:\n",
            "  Mean cosine similarity: 0.8494\n",
            "  Median cosine similarity: 0.8548\n",
            "  Std deviation: 0.0292\n",
            "  Min: 0.7857\n",
            "  Max: 0.8893\n",
            "\n",
            "Generating similarity distribution plot...\n",
            "✓ Saved similarity distribution to evaluation/similarity_distribution.png\n",
            "Generating top/worst predictions plot...\n",
            "✓ Saved top/worst predictions to evaluation/top_worst_predictions.png\n",
            "Generating PCA visualization...\n",
            "✓ Saved 2D embedding visualization to evaluation/embeddings_pca.png\n",
            "Generating t-SNE visualization...\n",
            "✓ Saved 2D embedding visualization to evaluation/embeddings_tsne.png\n",
            "\n",
            "Testing random characters...\n",
            "\n",
            "======================================================================\n",
            "TESTING CHARACTER: Shenhe\n",
            "======================================================================\n",
            "Image path: /content/drive/MyDrive/characters/Shenhe.png\n",
            "\n",
            "Character attributes:\n",
            "  character_name: Shenhe\n",
            "  region: Liyue\n",
            "  vision: Cryo\n",
            "  weapon_type: Polearm\n",
            "  body_figure: Tall Female\n",
            "  rarity: 5\n",
            "  constellation: Crista Doloris\n",
            "  affiliation: Cloud Retainer's Abode\n",
            "\n",
            "======================================================================\n",
            "RESULTS:\n",
            "======================================================================\n",
            "Cosine Similarity: 0.8657\n",
            "Embedding Distance (L2): 0.5182\n",
            "\n",
            "Finding 5 nearest neighbors to PREDICTED embedding...\n",
            "\n",
            "Top 5 most similar characters (based on predicted embedding):\n",
            "  1. Chongyun: 0.8831\n",
            "  2. Xingqiu: 0.8804\n",
            "  3. Rosaria: 0.8776\n",
            "  4. Keqing: 0.8733\n",
            "  5. Shenhe: 0.8657\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TESTING CHARACTER: Freminet\n",
            "======================================================================\n",
            "Image path: /content/drive/MyDrive/characters/Freminet.png\n",
            "\n",
            "Character attributes:\n",
            "  character_name: Freminet\n",
            "  region: Fontaine\n",
            "  vision: Cryo\n",
            "  weapon_type: Claymore\n",
            "  body_figure: Medium Male\n",
            "  rarity: 4\n",
            "  constellation: Automaton\n",
            "  affiliation: Hotel Bouffes d'ete\n",
            "\n",
            "======================================================================\n",
            "RESULTS:\n",
            "======================================================================\n",
            "Cosine Similarity: 0.8441\n",
            "Embedding Distance (L2): 0.5583\n",
            "\n",
            "Finding 5 nearest neighbors to PREDICTED embedding...\n",
            "\n",
            "Top 5 most similar characters (based on predicted embedding):\n",
            "  1. Chongyun: 0.8915\n",
            "  2. Rosaria: 0.8814\n",
            "  3. Xingqiu: 0.8799\n",
            "  4. Shenhe: 0.8707\n",
            "  5. Keqing: 0.8676\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TESTING CHARACTER: Raiden Shogun\n",
            "======================================================================\n",
            "Image path: /content/drive/MyDrive/characters/Raiden_Shogun.png\n",
            "\n",
            "Character attributes:\n",
            "  character_name: Raiden Shogun\n",
            "  region: Inazuma\n",
            "  vision: Electro\n",
            "  weapon_type: Polearm\n",
            "  body_figure: Tall Female\n",
            "  rarity: 5\n",
            "  constellation: Imperatrix Umbrosa\n",
            "  affiliation: Inazuma City\n",
            "\n",
            "======================================================================\n",
            "RESULTS:\n",
            "======================================================================\n",
            "Cosine Similarity: 0.8308\n",
            "Embedding Distance (L2): 0.5817\n",
            "\n",
            "Finding 5 nearest neighbors to PREDICTED embedding...\n",
            "\n",
            "Top 5 most similar characters (based on predicted embedding):\n",
            "  1. Chongyun: 0.8835\n",
            "  2. Xingqiu: 0.8777\n",
            "  3. Keqing: 0.8738\n",
            "  4. Rosaria: 0.8719\n",
            "  5. Cyno: 0.8636\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "✓ Evaluation complete! Results saved to evaluation\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TESTING CHARACTER: Raiden Shogun\n",
            "======================================================================\n",
            "Image path: /content/drive/MyDrive/characters/Raiden_Shogun.png\n",
            "\n",
            "Character attributes:\n",
            "  character_name: Raiden Shogun\n",
            "  region: Inazuma\n",
            "  vision: Electro\n",
            "  weapon_type: Polearm\n",
            "  body_figure: Tall Female\n",
            "  rarity: 5\n",
            "  constellation: Imperatrix Umbrosa\n",
            "  affiliation: Inazuma City\n",
            "\n",
            "======================================================================\n",
            "RESULTS:\n",
            "======================================================================\n",
            "Cosine Similarity: 0.8308\n",
            "Embedding Distance (L2): 0.5817\n",
            "\n",
            "Finding 5 nearest neighbors to PREDICTED embedding...\n",
            "\n",
            "Top 5 most similar characters (based on predicted embedding):\n",
            "  1. Chongyun: 0.8835\n",
            "  2. Xingqiu: 0.8777\n",
            "  3. Keqing: 0.8738\n",
            "  4. Rosaria: 0.8719\n",
            "  5. Cyno: 0.8636\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TEXT-TO-IMAGE RETRIEVAL TEST\n",
            "======================================================================\n",
            "\n",
            "Query: 'A pyro character from Liyue'\n",
            "----------------------------------------------------------------------\n",
            "Top 5 matches:\n",
            "  1. Lyney: 0.2684\n",
            "  2. Cyno: 0.2657\n",
            "  3. Klee: 0.2498\n",
            "  4. Lisa: 0.2464\n",
            "  5. Shenhe: 0.2393\n",
            "\n",
            "Query: 'An electro polearm user'\n",
            "----------------------------------------------------------------------\n",
            "Top 5 matches:\n",
            "  1. Freminet: 0.2389\n",
            "  2. Chongyun: 0.2369\n",
            "  3. Keqing: 0.2269\n",
            "  4. Kaveh: 0.2251\n",
            "  5. Neuvillette: 0.2230\n",
            "\n",
            "Query: 'A character from Mondstadt'\n",
            "----------------------------------------------------------------------\n",
            "Top 5 matches:\n",
            "  1. Chongyun: 0.2290\n",
            "  2. Xingqiu: 0.2284\n",
            "  3. Klee: 0.2278\n",
            "  4. Freminet: 0.2225\n",
            "  5. Lisa: 0.2166\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}